INDEX - CTRL + F "XX - SUMMARY" to jump to summarized paragraph on speciified topic.

AA		30 Nov 2023		Wireshark Traffic Analysis (WTA), ICMP
AB		30 Nov 2023		WTA, DNS
AC		30 Nov 2023		WTA, Cleartext Protocol Analysis, FTP
AD		30 Nov 2023		WTA, HTTP
AE		30 Nov 2023		WTA, HTTPS
AF      02 Dec 2023     THM, Day 2 Cyber Advent, Data Science + Cybersecurity, Python (Pandas and Matplotlib), Jupyter
AG      02 Dec 2023     Core Windows Processes, Task Manager
AH      03 Dec 2023     THM, Day 3 Cyber Advent, Brute-forcing with Hydra and Crunch
AI      03 Dec 2023     ELK (Elastic Search, Logstash, Kibana), and Beats
AJ      04 Dec 2023     Palo Alto Firewalls Class, Day 1, Lab 1
AK      04 Dec 2023     THM, Day 4 Cyber Advent, CeWL Brute-forcing and wfuzz for fuzzing
AL      05 Dec 2023     THM, Intro to Splunk and Incident Handling, Cyber Kill Chain Invesetigation
AM      06 Dec 2023     Palo ALto Day 3 & THM Investigating w/ Splunk Labs x2
AN        07 Dec 2023    Cloning Git repository to PC/Laptop via SSH


AA - Loaded PCAP file, performed a quick walkthrough of the 961 Packets loaded. I learned today that ICMP packets are typically smaller in size, i.e, 64 bytes, 126 for the initial pings, but I noticed that a small number of ICMP packets were 1070 bytes long and a few spread between ~300 - 1000. 

AA - A seperate point of interest was pointed out by the video guide. The intial pings were being sent out on the second every second. It was hard for me to distinguish a pattern after awhile, but very obvious at first. Further research shows that this regular interval between pings is an abnormal behavior, either malicious or begnign, but worth noting eitherway.

AA - Next I will investigate the payloads to see if anything of interest is shown in plaintext. Packet #46 had the largest payload yet. Lots of mention of diffie-hellman key exchange, SHA hashing algorithms, SSH RSA, and much more. Not sure what this indicates... so some research is warranted

AA - ChatGPT help break down the situation. The text I found in the large ICMP packet is typical language for SSH negotiation parameters. The client and server are working out the agreement for key exchange. What is strange is that ICMP is not the typically used to transport SSH data. Very much so noteworthy.

AA - Before I continue, I wanted to review the OSI model because I think it is easy to understand when seeing it laid out in Wireshark. All of the layers except for Layer 1 (Physical), Layer 5 (Session), and Layer 6 (Presentation) are not shown in Wireshark.
Layer 2 - Data Link Layer: Represented as "Ethernet" or other link-layer protocol
Layer 3 - Network Layer: Represented by "Internet Protocol"
Layer 4 - Transport Layer: Displayed as TCP or UDP
Layer 7 - Application Layer: examples include HTTPS, DNS, and ICMP.

AA - Back on topic. After the initial key exchange that was performed, most of the payloads were unreadable. In packet 132 I found "test local" which was consistently in packets from that point on. Research says this is also abnormal, which I had no clue it was. ICMP packets are generally used for control messages and error reporting. So to see "testlocal" in the payload signifies that it has been injected.

AA - SUMMARY. I determined the following indicators of compromise: Large size/payload ICMP packets, SSH and key exchange over ICMP, and deliberate inclusion of a string "test local" in the ICMP payload.

AB - New task and initial walkthrough. I see a multitude of different protocols, but the main protocol is DNS. I remember DNS as the phone book of the internet. I filter by dns because that is the topic. The queries start of small <200 bytes. I begin to notice amongst binary text the term 'datexfil com'. Supposedly this is abnormal, but I wouldn't have known that.

AB - Video guide suggested to right click "Name" under the DNS "Queries" and add it as a column so I can quickly identify any abnormalities. Well that was easy. I sort by smallest because most of the names are obfuscated and unreadable. Once sorted I scroll to the top and wallah! DNS Cat and the data exfil sites were right there. I learned that DNS Cat is a solid IoC because DNS Cat is used by attackers to establish covert communication between compromised systems and C2 servers (Command and Control).

AB - On a random note, I learned what "Defanging" is today. probably for the second time. More importantly is the purpose of defanging something. By putting the brackets around the period ([.]) I reduce the likelihood of this url or code being used accidently because it would have to be defanged prior.

AB - SUMMARY. When it comes to finding abnormal DNS traffic it is not as useful to sort by length. I learned that larger lengths mean larger payloads which should've been more obvious. I still benefited from going through the payloads quickly to look for any clear text IoCs. That's how I stumbled upon it before doing it the easier way which was by adding the DNS query name column. I will likely use this feature moving forward. I also understand defanging better.

AC - FTP has more clear text in the comments than the previous two protocols (ICMP and DNS) which makes spotting abnormalities in the info column easier. Initially I see one bad login attempt and then someone used the admin user name but did not provide a password. Keeping an eye for that. Now I can see the actual data being passwed into the password column. I also see that is failing to authenticate.

AC - So this seems to be a brute-force attack using words the attacker may have gathered from recon and social engineering, because they aren't your typical run of the mill passwords.

AC - The video guide recommends going into the FTP data on a failed login packet and drag/dropping the "Response arg: Login incorrect." text into the display filter so that we only see these packets. This provides us with the information that there were 737 brute-force login attempts! Restarted my AttackBox machine and it was suprisingly not slow yet.

AC - Now I use the following filter: ftp contains "ftp", to find all packets that have a mention of the word "ftp" in clear text. This is an easy trick with FTP given the nature of the plain text used in this protocol. In my parenthesis I could use a different username or password if I was interested. At this point I am going to filter down to TCP conversations. This is cool because now I can see that the attacker successfully logged in and I can see all of his communication with the CLI. I know exactly what he is doing now.

AC - SUMMARY. FTP can be an easier protocol to follow what's going on. I learned the importance of using filters and that I don't necessarily need to memorize filter, but rather drag and drop them or right click to apply them. The next valuable thing I learned here was how to follow TCP conversations and why. This allowed me to easily follow the conversation between the attacker and his target which is all thanks to FTP being unsecure.

AD - I'm trying to determine the # of different user-agents, which is located under the HTTP content information. I wanted to try doing this myself using the new techniques I've learned so I applied new filter using a single user agent and then changed the filter to be not-equal to that. This is probably inefficient. Yeah totally is. Next I tried to create a column for User-Agent and sort/count it that way, but I was not able to get the correct answer.

AD - SUMMARY. Video guide shows that I wasn't far off. I need to pay better attention the user-agent text especially towards the end of the name. I was seeing Mozilla and automatically thought it was an ok user agent. What I failed to see was the Mozilla was mispelt as Mozlila and also mentions the Nmap Scripting Engine which should all be red flags. Ultimately, I just didn't know what was an normal/abnormal user agent name. Video guide recommends extracting this pcap and using tshark to comb through unique user-agents in the CLI. I also learned that an attacks starting phase will be associated with a POST command, or the other way around. Inside the post I found the attackers attempt to chmod and run a shell. It was encoded in base64 so I used cyberchef to decode this. I learned how to use tools->credentials in Wireshark to have the software gather all the potential login credentials found in the PCAP.

AE - Startted of realigning columns to their content. Checked the data link layer of the "Client Hello" packet to find frame number. Next I needed to decrypt the HTTPS traffic using a conveinently had cipher key. To implement this into Wireshark I went to Edit -> Preferences -> Protocols -> TLS -> and uploaded the key I had into the "(Pre)-Master-Secret log filename.

AE - I know that the cipher key worked because now my Wireshark packets have either a HTTP2 or TLSv1.3 protocol listed in the respective column. HTTP2 means that the application layer data will now be in plain text and TLSv1.3 will show me the transport layer data, but unfortunately not the application layer stuff. I located the authority header under the application layer data. Next I used the search feature and looked for the THM flag by searching "flag{" and done.

AE - SUMMARY. I learned how to decipher HTTPS traffic once a key has been acquired. I am not sure how to come into possession of the key, but I am sure that will come in time. I learned how to identify decrypted HTTPS data as HTTP2 and practiced applying OSI networking terms like application layer. All good things.

AF - I had a feeling I would enjoy this task. I was first more interested in "Data Analytics" than I was in Cybersecurity. This was mostly because I didn't fully grasp the world of Cyber. I still haven't, but I know a whole lot more. I am excited to learn how data science can be applied to Cybersecurity. I think I will have a better idea of what direction I want to take after this room.

AF - Some of this is a review. I took a Python class in October and I am in a Coursera Python class too. Using Jupyiter I practiced python3 strings, variables, and lists. I imported panda via: import panda as pd, importing it as a smaller amount of text makes it easier to recall this tool. This was able to create a numbered list as long as I gave it the items and the column names, rows were automatically numbered. Next we used Matplotlib. To import we used: import matplotlib.pyplot as plt , again using a shorter name to call the tool later. Supposedly matplotlib acts wierd in Juypiter so I used the following command to keep it in line: %matplotlib inline . Next plt.plot() creates a line graph/plot.

AF - Now for the capstone challenge where I use everything I just learned about to include Python3, Panda, and MatPlotLib. First we import our libraries of course. Next I dentify the documents that I will be referencing and open up a fresh slate. First to aggregate the data I used:
    df = pd.read_csv('network_traffic.csv')
    df.head(5)
    #df stands for data frame and I use it as a variable set to use panda to read a csv file and specify the name of the file
    #df.head(5) calls my df variable to read the file specified and print the first 5 rows.
    next I use df.count()
    #again calling my df variable and using the count method to coun the number of rows for each column.
    next command: df.groupby(['Source']).size().sort_values(ascending=False)
    #this one is much more complicated. Calling the df variable and several methods. First we are grouping by the column called source which is the IP address column. Then we use the size method and sort values method to create a list showing us which IP addresses were listed the most times in the source document. We do this one more using the protocol column.

AF - SUMMARY. The video guide for this task spoke alot to the importance of using python and data science tools to become a master cyber analyst. This lesson reinforced that idea with me and I plan to spend more time. I learned about some python tools that can be imported for making data analytics and visualization easier. I am still not comfortable with using these tools without a guide but that will come with time. The tools I used today were Panda and MatPlotLib.

AG - I used stuffy24's video guide to pace me through the lesson. He believes this is an important lesson because being familiar with the baseline Windows processees will help me identify and abnormal processes a.k.a Indicator of Compromise. First I learned how to view Process ID #s in task manager along with other columns by right clicking the columns. I had used PIDs during the Metasploit rooms, specifically when using Meterpreter to take over processes/services.

AG - I selected all columns to be visible via right clicking column headers and I wanted to pay special attention to 'Type', 'Publisher', and 'Command Line' because these are the ones that are not defaulted on and therefore I am the least familiar with. I did this on my own computer instead of the VM because I wanted exposure to what was going on with my own machine. I noticed that some processes belonged to Microsoft as two separate entities to include Microsoft Windows and Microsoft Corporation. Adobe had some processes going which made me curious because I have no clue why Adobe is running. AMD processes made since with the CPU and GPU software running in the background. Discord had a surprising number of background processes as did Oculus which I don't use enough to allow that to consume resources. Moving on I noticed when it came to PID that the numbers seem to be assigned chronologically at boot and so on. PID 4 is always 'System'. Process types considered 'Apps' seem to be only things that I can actively see on my screen i.e., Chrome and Task Manager. Windows processes seem to be things that are necessary to allow the computer and operating system to do the things that they were designed to do. Seeing the command line language was interesting. One thing I draw from this is we can see if the process is being run from a legit place or not. This also reinforces how much more difficult Windows command line is compared to Linux, at least in my opinion. I think that I will definitely use Linux as my main OS when I build my next computer in many years. Distracted agian. Last I looked at the consumed resources data and noticed that Chrome consumes a lot of memory and so does the combination of many of my background tasks. I think that I will need to add 2 more 16gb RAM sticks in the future. Chrome is also CPU intensive and power intensive.

AG - The video guide mentions to pay attention to the Command Line info, for example a printer's exe file shouldn't have a command line prompt that calls to communicate with an server elsewhere. Stuff like that. We went over the Process Hacker tool which is a better task manager, but may not be available on a corporate end user machine. Same thing for Process Explorer. The video guide goes on to explain the importance of not being limited by tools and to have the skills to perform functions without special tools. Out of curiousity I asked GPT how to find parent processes without fancy tools.

AG - I can view parent processes using CMD and the following commands; wmic process where "Name='System'" get Caption,ParentProcessId,ProcessId. Another method would be to replace the Name=System part with ProcessID=7376 or some other number.

AG - SUMMARY. Some processes are only started by other processes. System always has a PID of 4. System is always the parent of smss.exe and anything else should be concerning. The process smss.exe will only show up in task manager when it is starting up another process and then it will disappear which is what is known as a non-existant process (when checking the parent). There should always be 2 instances of csrss.exe running known as session 0 and session 1. These processes should always have a non-existent parent. winlogon.exe is the ALT+CTRL+DELETE key combo used prior to entering the user and pass. Wininit.exe is started by smss.exe instance which disappears. Wininit.exe -> services.exe -> svchost.exe. Userinit.exe spawns explorer.exe and causes explorer.exe to have a non-existent parent.

AH - Ok this was awesome. I find brute-forcing to be very interesting and using Crunch/Hydra is new to me. First, I determined the maximum number of characters that could be used. In this case it was a pin pad so the minimum and maximum amount of characters were the same, 3. I had 0-9 and A-F as options so that gives us 16 possible characters over a 3 character pin. Crunch generated 4096 possiblities. The crunch command as follows: crunch 3 3 0123456789ABCDEF -o 3digits.txt This sets the min and max characters to 3, specifies the possibile characters and gives an output file that will be used later.

AH - Hydra time. Here is the command: hydra -l '' -P 3digits.txt -f -v 10.10.59.67 http-post-form "/login.php:pin=^PASS^:Access denied" -s 8000 . To break this dowm, -l '' tells hydra to login with no username because nothing was specified between the quotes. -P 3digits.txt tells hydra to use my crunch created password list. -f and -v tells hydra to stop when login is successfull and provide a verbose output should I need to troubleshoot. We include the IP address of the login portal, we specift that we are using HTTP post method to submit our password to the server, and the next part is a little tricky. "/login.php:pin=^PASS^:Access denied" is actually three sections separated by colons. When I view the access portal source code I can see that /login.php is the page where the password is submitted and pin=^PASS^ tells hydra to submit the passwords from my list at this point in the web transaction. We also include the text that would be displayed if the password failed, which is "Access denied". This works and I have entry.

AH - SUMMARY. Very fun box. I used crunch to create a password list that doesn't waste time with any password that are not possibilities. This required a small amount of enumeration like going to the website and seeing what the pin pad would. I used that password list that crunch made with Hydra and this Brute-forced the website in a very short ammount of time.

AI - Finally some good stuff after the boring Windows-based rooms that were putting me to sleep. Excited to learn about ELK because I've heard it mentioned several times in comparison to Splunk. I learned the process of which the ELK components work together to accomplish the end goal. Beats is the data collection agent, Logstash does the data input/filter/process/output, Elastic search indexes the data, and finally Kibana is the analysis and Visualization tool to gain the insight into what the data means and allows us to search the data that has been indexed.

AI - SUMMARY. I used Kibana to practice filtering my query to gain specific results. I found a way I think is easiest to filter. Say I want to find "John" and the only name I see is "Will". Well I can filter by Will and then edit the filter to say John. Easy day. I learned the Kibana Query Language. Honestly seemed very forgiving. Next was to create some visualizations. Here I was able to create visuals that would probably be appealing for presenting to others, but I think I'd rather have the data. Neat feature though.

AJ - Revisiting OSI again. A network switch is just a way to complete the circuit. In one connection and out another. Example is telephone operators connecting phone lines through the physical application (way back in the day). Data link layer is switching, uses MAC addresses. Network layer uses IP addresses to connect numerous networks, does routing. Transport layer is mainly made up of Transmission Control Protocol and User Datagram Protocol, uses ports. Being a Next Gen Firewall means you can do Layer 7 analysis which is also known as a DPI or Deep Packet Inspection to see the actual data that is being passed over a network.

AJ - Instructed to look into Kubernetes which is a way to run work loads in their own environments, knows as containers. New topic, discussed Zero Trust which is never trust; always verify. Inspect perimeter and internal traffic. Inspect north-south traffic to the internet as well as east-west traffic within the internal network, which is traffic between users and applications. I learned that firewalls are actuall hardware components, I always imagined it as a virtual component. That being said, a physical firewall can be partioned into several logical firewalls with its own set of policies.

AJ - Statefull firewalls package packets into sessions to inspect. The sessions are specific to conversations like a web browsing session. This gives the firewall more context as to what the session is related to. New sessions per second is a metric of how many sessions a firewall can handle at one time. Just loaded the first lab which is deploying a firewall. Started by applying a baseline configuration to the firewall. Unrelated reminder to myself (because it showed up on the firewall webpage), FQDN stands for fully qualified domain name. includes:
    Protocol: https://
    Subdomain: WWW.
    Domain Name: tryhackme
    Top Level Domain: .com
    Root Domain: tryhackme.com
    To look up a DNS use nslookup https://www.springschapel.org

AJ - MGT is the label for a management port on a firewall. Console is a port plug for using command line interface. This is called Remina in lab. I set the permitted IP address to manage the firewall is 192.168.0.0/16. This allows all IP addresses that start with 192.168 and subnet 255.255 to manage the firewall. Misconfiguration here can lose network connectivity to the firewall and would require special intervention.

AJ - SUMMARY. Great lesson. Reviewed network basics and gained a new perspective on the OSI model. Discussed the physical layer in more detail. Discussed what make a NGFW a NGFW and how Palo Alto (PA) is special here. Practiced setting up and updating the PA firewall.

AK - The goal is to gain access to a portal (http://10.10.114.184/login.php), using CeWL. This is the initial command we will be using: cewl -d 2 -m 5 -w passwords.txt http://10.10.114.184 --with-numbers . To break this down, -d is the depth which defaults at 2. The video guide explains this as the number of clicks that will need to be made on the page, hoping to clarify this more. Next is the -m flag which sets the mimimum word length. -w writes the output to the specified file and of course we include the target. --with-numbers tells cewl that its okay to include numbers. This generated 253 passwords which I determined using the wc -l passwords.txt command.

AK - I learned that CeWL creates lists based on information found on the website specified. Probably not a good idea to use credentials that are related to the web content. Anyway, I did the same thing to create a 37 username text file. I inspected the file with nano and realized that alot of the items were not probable usernames so I went ahead and edited it down to 21 names. With both my usernames and passwords lists created it was time to brute-force. However this was different than I have ever done before because now we are using wfuzz which is a fuzzing tool. At this point I should test the website using burp to see if the login page I was given is the definite location where credentials are passed via POST requests. It is, good to go.

AK - For the fuzzing uising wfuzz this was the command: wfuzz -c -z file,usernames.txt -z file,passwords.txt --hs "Please enter the correct credentials" -u http://10.10.114.184/login.php -d "username=FUZZ&password=FUZ2Z" . Break this down; -c tells wfuzz to output with colors, -z file,usernames.txt and passwords.txt sets it to scan mode meaning connection errors will be ignored and uses our generated lists. --hs "Please enter the correct credentials" is the error message when login fails so i use this to hide responses with this. -u specifies url and -d provides the POST data format where FUZZ will be replaced by the items in our wordlists. FUZ2Z is not a typo, it specifies the order.

AK - SUMMARY. I used CeWL almost as a OSINT harvesting tool for generating wordlist. Then used wfuzz to insert wordlist items into the POST command and brute-force more or less a web portal. I learned how important editing the wordlists are. I thought I did ok, but it took 5566 attempts compared to the video guide who sent 508 requests. I think both would stand-out to blue-team but mine was far worse. This stresses the importance of carefully selecting usernames and passwords. I learned that this was possible because there were restrictions imposed on the login portal like number of attempts etc. I originally thought it was related to the portal being a .php but that was not the case. I learned alternatives to wfuzz are ffuf which might have a better syntax in my opinion. There is also Hydra which we used yesterday. Hydra looks as difficult as wfuzz. Burpsuite is a final option provided by the video guide. Problem is burp community edition is too slow for intruder, there is a free extension called turbo intruder which solves this. Made by the same people who make burp LOL.

AL - Started the morning going over Splunk Enterprise on THM. Loaded a .json file for some VPN logs and used filters to find specific data. Initial thoughts are I found Kibana slightly easier in terms of overall interface, but I know with practice this will become easier. Patrick talked about using API and python to automate things with the firewall. To get there we used http://192.168.1.254/api. Had several conversation about authentication protocols that are options in the firewall such as LDAP, TACACS+, Kerberos, RADIUS, and more. Struggling to focus on the firewall class, mostly my fault, sort of the instructor going off on tangents that I don't want to follow. Back to the THM Splunk Incident Handling Room

AL - I take the given web server to plug into the search query and also filter by http traffic with stream:http. 20k results. I check the source IPs communicating to my server and I get two main IPs (40.80* and 23.22*) with the vast majority of traffic from the former. I filter down to the loud IP and use suricata to see what alerts generated. Of interest there was a CVE-2014-6271. Research via MITRE shows this CVE is known as "Shellshock" that was a bash vulnerability. Spoiler-alert, this lab deals with a brut-force attack against a web-server access-portal so I don't think it is related to the CVE, but worth investigating either way. I continued to add filters that I thought would help me get to the bottom of what was going. Alright, lets be real...tryhackme told me what to do, but I understand what is going on, I think? ALl of the traffic we investigated up to this point were all indicators of the attacker going through the recon phase of the cyber kill chain.

AL - I filtered down to POST methods over HTTP and started looking into the common requests and that's where I found: <?php echo(md5(acunetix-php-cgi-rce)); ?>. The lab doesn't talk about this but I thought the combination of php code and the echoing of a md5 hash (of an unknown item) was suspicious. ChatGPT said this was probably malicious, but not entirely trusting of this. Moving on, I look at the URI results and see a uri="/joomla/administrator/index.php" which I look into by going to that webpage which happens to be the logon portal. I create a query to generate statistics which well help me analyze the data, which leads to a discovery of a brute-force attack from the unsuspecting 23.22* IP address. We regex the data to make it even easier to read usernames and passwords (which are sadly passed over in cleartext btw) and I see the attacker is attempting to take the admin account. I learned that Regex stands for regular expressions. I used this to pull just the password values. rex field=form_data "passwd=(?<creds>\w+)" does this and now I have a column with just usernames. Next we investigate the user agents and discover that a python script was being used for the brute forcing. We find that the final password attempted was 'batman' before the brute-force stopped and then the 40.80* IP with the Mozilla user-agent logs in with batman. So there is 1 attacker and two ip addresses (one associated with the password cracker and one associated with the device the attacker uses to log in). The attacker has successfully completed the exploitation phase.

AL - Now to figure out what the attacker did once they gained access. Updating the filter, we look for any .exe files that were used and include the "part_filename{} field to find a filed called 3791.exe no bueno. Looking at the payload I found some plaintext that says, "This program cannot be run in DOS mode." The http method was also POST which tells us this .exe was uploaded to the server. I start breaking away from the thm guide at this point. I notice the .exe was uploaded as an image from an Image Loaded packet. I also see the MD5 hash in this packet so I'll lookup if it is a known malicious file on VirusTotal which provided me with an astounding yes this is in-fact a malicious file (Malware Trojan). Some of the MITRE ATT&CK TTPs used are Privilege Escalation, Defensive Evasion, Credential Access, Discovery, Collection, and C2. It is capable of data-manipulation, host-interaction, and executable. One of the most interesting things that I found is the decoded text under the behavior tabs (VirusTotal) says {"Type": "Metasploit Connect", "IP": "23.22.63.114", "Port": 3791} which is interesting because in my scenario that same IP was the browser that logged into the admin page and the .exe was name 3791 which is the port here. Interesting stuff!

AL - Some more snooping I find that cmd.exe was ran in the command line and the parent process was the malware .exe. I verified this was cmd.exe with the file hash. It was at this point I learned that Splunk Events are not the same thing as packets. Splunk Events can be packets should that be the source but they can come from Windows_Event_signatures. One thing that is really stumping me is how the user account associated with executing the malware is the NT AUTHORITY\IUSR account. Back to the THM guide. I learned that Event ID: 1 is associated with creation a new process and there was an event for when the malware was executed. We have proof that the threat actor has now completed the installation part of the cyber kill chain. Next we try to figure out how what the attacker did with the backdoor they installed. I only know its a back door because THM said so, nothing on VirusTotal pointed to that. The guide shows me that when I filter down to suricata traffic that has the web server as the destination that there were no external communications. In other words all of the events generated by suricata show only communications internal to the network when we were mostly concerned with external comms to the internal network. When I reverse the filter to show the web server as the source IP, we now see the web server communicating with the threat actors which is peculiar for sure. I continued narrowing down the filter and eventually found a jpeg file that I haven't seen so far in this THM room and the file name is suspect. I found this via Url filtering. We found the server from which the jpeg was downloaded. I learned you can pause a query if it is generating alot of traffic and messing up operations. During the Command and Control phase investigation the main thing I learned is that it may be necessary to keep certain parameters in the filter the same while changing a single one, for example changing the source type back to htttp to see web traffic.

AL - SUMMARY. This was a beast. Today I learned to use the Splunk Enterprise search / query / filter features to find suspicious activity. What filters I found most  helpful are: source / sourcetype because this shows you where the log came from, whether it be HTTP traffic or the IDS triggered on some traffic. The rest of the filters will change drastically depending on what you modify here so it's important to be familiar with the possible types so that you can evaluate the contents and understand what is happening. For example, knowing what the POST method means with HTTP / HTTPS traffic or knowing that suricata is an IDS so I might want to filter down the event_type. There is endless combinations and in the end I will get better by experimenting and learning. I learned why documenting the incident and all of your finding is important and how to do that a little better. I studied the cyber kill chain more and tried to apply it to what was happening. Great lab.

AM - Learned that RFCs or request for comments are the "rules for the internet". RFC 1918 February 1996 established Private IP Address Space. Anything that starts with 10.10, 172.16**, 192.168** are reserved for private networks. Because of concerns of running out of addresses we began using subnets. It was cool to see the source documentation for how this was established in my birth year.

AM - Today using Splunk I had much less direction. I needed to set the index on my own. I have an idea. I'm going to take notes on here and roleplay as an analyst or atleast how I imagine one to be. All of the events are from an event_log but no details beyond that. The layout of the alerts suggests it's from a PCAP or Sysmon. It doesn't look like the alerts from yesterday. There are 2 NT AUTHORITY users which make up ~75% of the traffic. The other 2 are Cybertees users, whatever that is. I think it is intersting 98% of traffic is from account name System and then a small amount of traffic from James. The most ran application is svchost.exe which may not be strange because it acts as a common parent process for other services, but 61% seems excessive. I noticed in the category for alert there was a "File Created" filter with not a lot of traffic, but I was curious. Once I filtered this on I checked host name to see who was doing this and James.Browne accounted for 92% of the occurences. I also noticed that the top 3 images associated with these were svchost.exe, BackgroundTransferHost.exe, and backgroundTaskHost.exe. I asked GPT about why the b in background wasn't capitol in the third .exe and learned that Windows file systems are case-insensitive whcih I think is stupid because that would be an easy IoC. Oh well. I went through alert categories and a few of them stood out to me to include Handle Manipulation and Process Accessed. My task says to find out what account the threat actor created. I struggled with this one, but ChatGPT informed me the EventID for a new account is 4720 which led to the answer. I didn't know prior that eventID #s are set in stone by Windows. At this point I was pretty stumped so I pulled up a guide. The next thing I needed to was to filter EventID 1 and view the command line field which gave me the executed remote code. ID code 3 was used to show us how when new network connections are logged which would have shown me if the backdoor was used. Event ID 4103 is logged for whenever powershell is enabled. Finished Lab 1.

AM - Limited to only Event ID 4688 which is used to identify the start of a new process creation. I struggled very hard with this room. I learned through some guides some useful ways to use piping in Splunk to create statistics and table such as | stats count by UserName and | table UserName | dedup UserName. I used multiple guides and they all said I needed to know that schtasks.exe is a normal Windows executable for running scheduled tasks. The question was Which user from the HR department was observed to be running scheduled tasks? This drove me nuts because no way do all these people know that, cheaters. I learned a new term today LOLBIN which stands for Living off the land binaries which are binaries that are not inherently malicious and they are local to the OS, but attackers use them to bypass detection.

AM - SUMMARY. Today was tough. What I learned: Learned the reserved private IP addresses. I tried to investigate on my own, but ultimately today came down to knowning about Event IDs which have more meaning than I suspected. FIltering and researching these was all I needed in the first room. The second room was nearly opposite because we were limited to one ID. I got more practice with using the statistics and visualization tools within Splunk. The second room required more knowledge about LOLBINs and now I have a good resource for that. I had to do a lot of filtering by ProcessName and I wish I knew a better way. Good challenge.

AN - I made a new repository for the Springs Chapel project on GitHub.com. Now I want to clone the repository to both my PC and laptop so I can edit it on the road and at home and the pastor will have the ability to edit it without version control issues. I have the Springs Chapel (SC) folder on my desktop so I modifed my directory location in powershell (PS) to be there. Now to use git clone and the command generated on the web-based repository. My computer warns me that authenticity can't be established because this is the first time connecting to it. I am given the SHA256 hash of a public key which I learned that the key is generated using the ED25519 asymmetric cryptography a.k.a Edwards-curve Digital Signature Algorithm. 25519 refers to the prime number 2^255 - 19 which defines the specific elliptic curve. I'll check the hash against the website's fingerprint registry. Using powershell is a more difficult method of comparing hashes then I expected, so I am going to do it in Python instead. I create two variables for each key. Then I am sure to remove spaces or tabs a.k.a whitespace characters. They matched. I stored the GitHub (GH) fingerprint but my public key is not associated with my GH account so now I generated a new key and associated the email associated with my GH account. ssh-keygen -t ed25519 -C tannerlayton01@gmail.com. It created a really interesting looking randomart image for my key's fingerprint. I then went into my account settings and added the SSH key. Then back to PS and I used ssh -T git@github.com to check that my update worked and it did! I got an error message just because the repo was empty on the website but now I will create a document via PS. echo "# SpringsChapel" >> README.md
git add README.md
git commit -m "Initial commit"
git push origin main
Now I will open the current folder in VS Code (VSC) to edit the readme document to test the pushing of the document to the main repository. Now to push the document back to the online repository I use the following commands: 
git add README.md
git commit -m "a second trial, nothing to see here"
git push origin main
Now I am doing the same process on my laptop.







