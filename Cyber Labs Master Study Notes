INDEX - CTRL + F "XX - SUMMARY" to jump to summarized paragraph on speciified topic.

AA		30 Nov 2023		Wireshark Traffic Analysis (WTA), ICMP
AB		30 Nov 2023		WTA, DNS
AC		30 Nov 2023		WTA, Cleartext Protocol Analysis, FTP
AD		30 Nov 2023		WTA, HTTP
AE		30 Nov 2023		WTA, HTTPS
AF      02 Dec 2023     THM, Day 2 Cyber Advent, Data Science + Cybersecurity, Python (Pandas and Matplotlib), Jupyter
AG      02 Dec 2023     Core Windows Processes, Task Manager
AH      03 Dec 2023     THM, Day 3 Cyber Advent, Brute-forcing with Hydra and Crunch
AI      03 Dec 2023     ELK (Elastic Search, Logstash, Kibana), and Beats
AJ      


AA - Loaded PCAP file, performed a quick walkthrough of the 961 Packets loaded. I learned today that ICMP packets are typically smaller in size, i.e, 64 bytes, 126 for the initial pings, but I noticed that a small number of ICMP packets were 1070 bytes long and a few spread between ~300 - 1000. 

AA - A seperate point of interest was pointed out by the video guide. The intial pings were being sent out on the second every second. It was hard for me to distinguish a pattern after awhile, but very obvious at first. Further research shows that this regular interval between pings is an abnormal behavior, either malicious or begnign, but worth noting eitherway.

AA - Next I will investigate the payloads to see if anything of interest is shown in plaintext. Packet #46 had the largest payload yet. Lots of mention of diffie-hellman key exchange, SHA hashing algorithms, SSH RSA, and much more. Not sure what this indicates... so some research is warranted

AA - ChatGPT help break down the situation. The text I found in the large ICMP packet is typical language for SSH negotiation parameters. The client and server are working out the agreement for key exchange. What is strange is that ICMP is not the typically used to transport SSH data. Very much so noteworthy.

AA - Before I continue, I wanted to review the OSI model because I think it is easy to understand when seeing it laid out in Wireshark. All of the layers except for Layer 1 (Physical), Layer 5 (Session), and Layer 6 (Presentation) are not shown in Wireshark.
Layer 2 - Data Link Layer: Represented as "Ethernet" or other link-layer protocol
Layer 3 - Network Layer: Represented by "Internet Protocol"
Layer 4 - Transport Layer: Displayed as TCP or UDP
Layer 7 - Application Layer: examples include HTTPS, DNS, and ICMP.

AA - Back on topic. After the initial key exchange that was performed, most of the payloads were unreadable. In packet 132 I found "test local" which was consistently in packets from that point on. Research says this is also abnormal, which I had no clue it was. ICMP packets are generally used for control messages and error reporting. So to see "testlocal" in the payload signifies that it has been injected.

AA - SUMMARY. I determined the following indicators of compromise: Large size/payload ICMP packets, SSH and key exchange over ICMP, and deliberate inclusion of a string "test local" in the ICMP payload.

AB - New task and initial walkthrough. I see a multitude of different protocols, but the main protocol is DNS. I remember DNS as the phone book of the internet. I filter by dns because that is the topic. The queries start of small <200 bytes. I begin to notice amongst binary text the term 'datexfil com". Supposedly this is abnormal, but I wouldn't have known that.

AB - Video guide suggested to right click "Name" under the DNS "Queries" and add it as a column so I can quickly identify any abnormalities. Well that was easy. I sort by smallest because most of the names are obfuscated and unreadable. Once sorted I scroll to the top and wallah! DNS Cat and the data exfil sites were right there. I learned that DNS Cat is a solid IoC because DNS Cat is used by attackers to establish covert communication between compromised systems and C2 servers (Command and Control).

AB - On a random note, I learned what "Defanging" is today. probably for the second time. More importantly is the purpose of defanging something. By putting the brackets around the period ([.]) I reduce the likelihood of this url or code being used accidently because it would have to be defanged prior.

AB - SUMMARY. When it comes to finding abnormal DNS traffic it is not as useful to sort by length. I learned that larger lengths mean larger payloads which should've been more obvious. I still benefited from going through the payloads quickly to look for any clear text IoCs. That's how I stumbled upon it before doing it the easier way which was by adding the DNS query name column. I will likely use this feature moving forward. I also understand defanging better.

AC - FTP has more clear text in the comments than the previous two protocols (ICMP and DNS) which makes spotting abnormalities in the info column easier. Initially I see one bad login attempt and then someone used the admin user name but did not provide a password. Keeping an eye for that. Now I can see the actual data being passwed into the password column. I also see that is failing to authenticate.

AC - So this seems to be a brute-force attack using words the attacker may have gathered from recon and social engineering, because they aren't your typical run of the mill passwords.

AC - The video guide recommends going into the FTP data on a failed login packet and drag/dropping the "Response arg: Login incorrect." text into the display filter so that we only see these packets. This provides us with the information that there were 737 brute-force login attempts! Restarted my AttackBox machine and it was suprisingly not slow yet.

AC - Now I use the following filter: ftp contains "ftp", to find all packets that have a mention of the word "ftp" in clear text. This is an easy trick with FTP given the nature of the plain text used in this protocol. In my parenthesis I could use a different username or password if I was interested. At this point I am going to filter down to TCP conversations. This is cool because now I can see that the attacker successfully logged in and I can see all of his communication with the CLI. I know exactly what he is doing now.

AC - SUMMARY. FTP can be an easier protocol to follow what's going on. I learned the importance of using filters and that I don't necessarily need to memorize filter, but rather drag and drop them or right click to apply them. The next valuable thing I learned here was how to follow TCP conversations and why. This allowed me to easily follow the conversation between the attacker and his target which is all thanks to FTP being unsecure.

AD - I'm trying to determine the # of different user-agents, which is located under the HTTP content information. I wanted to try doing this myself using the new techniques I've learned so I applied new filter using a single user agent and then changed the filter to be not-equal to that. This is probably inefficient. Yeah totally is. Next I tried to create a column for User-Agent and sort/count it that way, but I was not able to get the correct answer.

AD - SUMMARY. Video guide shows that I wasn't far off. I need to pay better attention the user-agent text especially towards the end of the name. I was seeing Mozilla and automatically thought it was an ok user agent. What I failed to see was the Mozilla was mispelt as Mozlila and also mentions the Nmap Scripting Engine which should all be red flags. Ultimately, I just didn't know what was an normal/abnormal user agent name. Video guide recommends extracting this pcap and using tshark to comb through unique user-agents in the CLI. I also learned that an attacks starting phase will be associated with a POST command, or the other way around. Inside the post I found the attackers attempt to chmod and run a shell. It was encoded in base64 so I used cyberchef to decode this. I learned how to use tools->credentials in Wireshark to have the software gather all the potential login credentials found in the PCAP.

AE - Startted of realigning columns to their content. Checked the data link layer of the "Client Hello" packet to find frame number. Next I needed to decrypt the HTTPS traffic using a conveinently had cipher key. To implement this into Wireshark I went to Edit -> Preferences -> Protocols -> TLS -> and uploaded the key I had into the "(Pre)-Master-Secret log filename.

AE - I know that the cipher key worked because now my Wireshark packets have either a HTTP2 or TLSv1.3 protocol listed in the respective column. HTTP2 means that the application layer data will now be in plain text and TLSv1.3 will show me the transport layer data, but unfortunately not the application layer stuff. I located the authority header under the application layer data. Next I used the search feature and looked for the THM flag by searching "flag{" and done.

AE - SUMMARY. I learned how to decipher HTTPS traffic once a key has been acquired. I am not sure how to come into possession of the key, but I am sure that will come in time. I learned how to identify decrypted HTTPS data as HTTP2 and practiced applying OSI networking terms like application layer. All good things.

AF - I had a feeling I would enjoy this task. I was first more interested in "Data Analytics" than I was in Cybersecurity. This was mostly because I didn't fully grasp the world of Cyber. I still haven't, but I know a whole lot more. I am excited to learn how data science can be applied to Cybersecurity. I think I will have a better idea of what direction I want to take after this room.

AF - Some of this is a review. I took a Python class in October and I am in a Coursera Python class too. Using Jupyiter I practiced python3 strings, variables, and lists. I imported panda via: import panda as pd, importing it as a smaller amount of text makes it easier to recall this tool. This was able to create a numbered list as long as I gave it the items and the column names, rows were automatically numbered. Next we used Matplotlib. To import we used: import matplotlib.pyplot as plt , again using a shorter name to call the tool later. Supposedly matplotlib acts wierd in Juypiter so I used the following command to keep it in line: %matplotlib inline . Next plt.plot() creates a line graph/plot.

AF - Now for the capstone challenge where I use everything I just learned about to include Python3, Panda, and MatPlotLib. First we import our libraries of course. Next I dentify the documents that I will be referencing and open up a fresh slate. First to aggregate the data I used:
    df = pd.read_csv('network_traffic.csv')
    df.head(5)
    #df stands for data frame and I use it as a variable set to use panda to read a csv file and specify the name of the file
    #df.head(5) calls my df variable to read the file specified and print the first 5 rows.
    next I use df.count()
    #again calling my df variable and using the count method to coun the number of rows for each column.
    next command: df.groupby(['Source']).size().sort_values(ascending=False)
    #this one is much more complicated. Calling the df variable and several methods. First we are grouping by the column called source which is the IP address column. Then we use the size method and sort values method to create a list showing us which IP addresses were listed the most times in the source document. We do this one more using the protocol column.

AF - SUMMARY. The video guide for this task spoke alot to the importance of using python and data science tools to become a master cyber analyst. This lesson reinforced that idea with me and I plan to spend more time. I learned about some python tools that can be imported for making data analytics and visualization easier. I am still not comfortable with using these tools without a guide but that will come with time. The tools I used today were Panda and MatPlotLib.

AG - I used stuffy24's video guide to pace me through the lesson. He believes this is an important lesson because being familiar with the baseline Windows processees will help me identify and abnormal processes a.k.a Indicator of Compromise. First I learned how to view Process ID #s in task manager along with other columns by right clicking the columns. I had used PIDs during the Metasploit rooms, specifically when using Meterpreter to take over processes/services.

AG - I selected all columns to be visible via right clicking column headers and I wanted to pay special attention to 'Type', 'Publisher', and 'Command Line' because these are the ones that are not defaulted on and therefore I am the least familiar with. I did this on my own computer instead of the VM because I wanted exposure to what was going on with my own machine. I noticed that some processes belonged to Microsoft as two separate entities to include Microsoft Windows and Microsoft Corporation. Adobe had some processes going which made me curious because I have no clue why Adobe is running. AMD processes made since with the CPU and GPU software running in the background. Discord had a surprising number of background processes as did Oculus which I don't use enough to allow that to consume resources. Moving on I noticed when it came to PID that the numbers seem to be assigned chronologically at boot and so on. PID 4 is always 'System'. Process types considered 'Apps' seem to be only things that I can actively see on my screen i.e., Chrome and Task Manager. Windows processes seem to be things that are necessary to allow the computer and operating system to do the things that they were designed to do. Seeing the command line language was interesting. One thing I draw from this is we can see if the process is being run from a legit place or not. This also reinforces how much more difficult Windows command line is compared to Linux, at least in my opinion. I think that I will definitely use Linux as my main OS when I build my next computer in many years. Distracted agian. Last I looked at the consumed resources data and noticed that Chrome consumes a lot of memory and so does the combination of many of my background tasks. I think that I will need to add 2 more 16gb RAM sticks in the future. Chrome is also CPU intensive and power intensive.

AG - The video guide mentions to pay attention to the Command Line info, for example a printer's exe file shouldn't have a command line prompt that calls to communicate with an server elsewhere. Stuff like that. We went over the Process Hacker tool which is a better task manager, but may not be available on a corporate end user machine. Same thing for Process Explorer. The video guide goes on to explain the importance of not being limited by tools and to have the skills to perform functions without special tools. Out of curiousity I asked GPT how to find parent processes without fancy tools.

AG - I can view parent processes using CMD and the following commands; wmic process where "Name='System'" get Caption,ParentProcessId,ProcessId. Another method would be to replace the Name=System part with ProcessID=7376 or some other number.

AG - SUMMARY. Some processes are only started by other processes. System always has a PID of 4. System is always the parent of smss.exe and anything else should be concerning. The process smss.exe will only show up in task manager when it is starting up another process and then it will disappear which is what is known as a non-existant process (when checking the parent). There should always be 2 instances of csrss.exe running known as session 0 and session 1. These processes should always have a non-existent parent. winlogon.exe is the ALT+CTRL+DELETE key combo used prior to entering the user and pass. Wininit.exe is started by smss.exe instance which disappears. Wininit.exe -> services.exe -> svchost.exe. Userinit.exe spawns explorer.exe and causes explorer.exe to have a non-existent parent.

AH - Ok this was awesome. I find brute-forcing to be very interesting and using Crunch/Hydra is new to me. First, I determined the maximum number of characters that could be used. In this case it was a pin pad so the minimum and maximum amount of characters were the same, 3. I had 0-9 and A-F as options so that gives us 16 possible characters over a 3 character pin. Crunch generated 4096 possiblities. The crunch command as follows: crunch 3 3 0123456789ABCDEF -o 3digits.txt This sets the min and max characters to 3, specifies the possibile characters and gives an output file that will be used later.

AH - Hydra time. Here is the command: hydra -l '' -P 3digits.txt -f -v 10.10.59.67 http-post-form "/login.php:pin=^PASS^:Access denied" -s 8000 . To break this dowm, -l '' tells hydra to login with no username because nothing was specified between the quotes. -P 3digits.txt tells hydra to use my crunch created password list. -f and -v tells hydra to stop when login is successfull and provide a verbose output should I need to troubleshoot. We include the IP address of the login portal, we specift that we are using HTTP post method to submit our password to the server, and the next part is a little tricky. "/login.php:pin=^PASS^:Access denied" is actually three sections separated by colons. When I view the access portal source code I can see that /login.php is the page where the password is submitted and pin=^PASS^ tells hydra to submit the passwords from my list at this point in the web transaction. We also include the text that would be displayed if the password failed, which is "Access denied". This works and I have entry.

AH - SUMMARY. Very fun box. I used crunch to create a password list that doesn't waste time with any password that are not possibilities. This required a small amount of enumeration like going to the website and seeing what the pin pad would. I used that password list that crunch made with Hydra and this Brute-forced the website in a very short ammount of time.

AI - Finally some good stuff after the boring Windows-based rooms that were putting me to sleep. Excited to learn about ELK because I've heard it mentioned several times in comparison to Splunk. I learned the process of which the ELK components work together to accomplish the end goal. Beats is the data collection agent, Logstash does the data input/filter/process/output, Elastic search indexes the data, and finally Kibana is the analysis and Visualization tool to gain the insight into what the data means and allows us to search the data that has been indexed.

AI - SUMMARY. I used Kibana to practice filtering my query to gain specific results. I found a way I think is easiest to filter. Say I want to find "John" and the only name I see is "Will". Well I can filter by Will and then edit the filter to say John. Easy day. I learned the Kibana Query Language. Honestly seemed very forgiving. Next was to create some visualizations. Here I was able to create visuals that would probably be appealing for presenting to others, but I think I'd rather have the data. Neat feature though.

AJ - 











